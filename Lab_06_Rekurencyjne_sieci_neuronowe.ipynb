{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "<font size=\"5\">\n",
    "\n",
    "Laboratorium z przedmiotu: \\\n",
    "**Głębokie uczenie i analiza obrazów**\n",
    "\n",
    "Ćwiczenie 6: \\\n",
    "**Rekurencyjne sieci neuronowe**\n",
    "\n",
    "</font>\n",
    "\n",
    "\\\n",
    "Marta Szarmach \\\n",
    "Zakład Telekomunikacji Morskiej \\\n",
    "Wydział Elektryczny \\\n",
    "Uniwersytet Morski w Gdyni\n",
    "\n",
    "11.2023\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "# 1. Wprowadzenie\n",
    "\n",
    "**Rekurencyjne sieci neuronowe** (ang. *Recurrent Neural Networks*, RNN) to takie sieci neuronowe, które dokonują w danej chwili $t$ predykcji nie tylko na podstawie ,,obecnych'' danych ($x_t$), ale też ,,wcześniejszych' (na podstawie zawartości **ukrytego stanu** $h_{t-1}$ zależącego od danych z wcześniejszych chwil $x_{t-1}$):\n",
    "\\begin{equation*}\n",
    "    h_t = f(h_{t-1},x_t) = \\textrm{tanh} \\left( \\Theta_h h_{t-1} + \\Theta_x x_t + b \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "To powoduje, że rekurencyjne sieci neuronowe świetnie sprawdzają się w analizie **sekwencji danych**, tj. danych składających się z elementów z różnych kroków czasowych, np.: fragmentów filmu (składające się z następujących po sobie ramek), zdań (składające się z następujących po sobie wyrazów), wycinków mowy czy muzyki. Modele te mogą:\n",
    "* analizować wejściową sekwencję danych i zwracać pojedynczą wartość (modele *many-to-one*), np. analizować sentyment tekstu, \n",
    "* analizować wejściową sekwencję danych i zwracać inną sekwencję (modele *many-to-many*, inaczej nazywane seq2seq), np. dokonywać tłumaczenia zdań, \n",
    "* na podstawie pojedynczej danej wejściowej generować całkowicie nowe sekwencje (modele *one-to-many*), np. generować słowny opis do zdjęcia.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/Argenni/GUiAO_lab/c2ecb6dd39a2ca46e3feff4fe513c09d36bc0270/rys/10_seq2seq.png'/>\n",
    "\n",
    "<font size=\"1\">Przykład modelu *many-to-many*. Grafika: Justin Johnson, University of Michigan</font>\n",
    "</div>\n",
    "\n",
    "Klasyczne sieci rekurencyjne borykają się jednak z pewnymi problemami. Mimo że formalnie nie posiadają ograniczeń co do długości analizowanej sekwencji danych, w rzeczywistości analiza długich sekwencji sprawia im problemy (występują takie zjawiska, jak *vanishing/exploding gradient*), modele gubią szerszy kontekst sekwencji. Aby sobie z tym radzić, proponuje się udoskonalenie klasycznych RNNów o następujące mechanizmy:\n",
    "* **context vector** $c_t$ - dodatkowa pamięć (mogąca różnić się od *hidden state*), przechowująca szerszy kontekst (np. informację o płci podmiotu w generowanym zdaniu), dopóki jest on potrzebny,\n",
    "* **bramki** (ang. *gates*) - współczynniki modyfikujące sposób wyznaczania $h_t$:\n",
    "    * *update gate* - decyduje, jak silnie opierać się na dotychczasowych danych podczas uaktualniania *hidden state* $h_t$ lub $c_t$,\n",
    "    * *forget gate* lub *reset gate* - decyduje, które dane z przeszłości zapomnieć (czy wyzerować $c_t$ lub $h_t$),\n",
    "    * *output gate* - decyduje, jak silnie opierać się na kontekście $c_t$ przy uaktualnianiu $h_t$.\n",
    "\n",
    "Przykładem zmodyfikowanej sieci RNN są sieci **GRU** (ang. *Gated Recurrent Unit*, wykorzystujące *update gate* i *reset gate*) oraz **LSTM** (ang. *Long Short-Term Memory*, wykorzystujące wszystkie bramki oraz pamięć o szerszym kontekście $c_t$).\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/Argenni/GUiAO_lab/main/rys/11_gru%26lstm.png'/>\n",
    "\n",
    "<font size=\"1\">Grafika: medium.com</font>\n",
    "</div>\n",
    "\n",
    "Innym udoskonaleniem modeli opartych na RNNach było wprowadzenie **atencji**, czyli mechanizmu decydującego o tym, które z danych z przeszłości mają większy wpływ na dokonanie predykcji w obecnym kroku (np. podczas dokonywania tłumaczenia zdania, większa uwaga poświęcana była wyrazowi aktualnie tłumaczonemu, nawet przy zmianie szyku zdania). Wprowadzenie mechanizmu atencji spowodowało powstanie niezwykle silnych modeli (takich jak BERT czy GPT), opartych na **transformerach**, realizujących zadania z zakresu **przetwarzania języka naturalnego** (obejmujące analizę tekstu i mowy ludzkiej), takie jak m.in.:\n",
    "* tłumaczenie zdań z jednego języka w drugi,\n",
    "* uzupełnianie brakujących słów/liter w tekście, generacja tekstu,\n",
    "* rozpoznawanie mowy i tekstu (OCR),\n",
    "* streszczanie długich tekstów, wyszukiwanie sentymentu.\n",
    "\n",
    "\n",
    "# 2. Cel ćwiczenia\n",
    "\n",
    "**Celem niniejszego ćwiczenia** jest zapoznanie się z działaniem rekurencyjnych sieci neuronowych poprzez implementację prostej sieci CharRNN, służącej do generacji tekstu znak po znaku, z wykorzystaniem języka Python i biblioteki PyTorch.\n",
    "\n",
    "\n",
    "# 3. Stanowisko laboratoryjne\n",
    "\n",
    "Do wykonania niniejszego ćwiczenia niezbędne jest stanowisko laboratoryjne, składające się z komputera klasy PC z zainstalowanym oprogramowaniem:\n",
    "* językiem programowania Python (w wersji 3.8),\n",
    "* IDE obsługującym pliki Jupyter Notebook (np. Visual Studio Code z rozszerzeniem ipykernel).\n",
    "\n",
    "\n",
    "# 4. Przebieg ćwiczenia\n",
    "## Generacja nowych łacińskich nazw roślin z wykorzystaniem CharRNN w PyTorch\n",
    "\n",
    "W ramach tego ćwiczenia, silnie opierać się będziemy na kodzie z [TEGO](https://github.com/nikhilbarhate99/Char-RNN-PyTorch/blob/master/CharRNN.py) repozytorium. Pobawimy się trochę w biologów/lingwistów: naszym zadaniem będzie zaprojektowanie rekurencyjnej sieci neuronowej, która nauczy się generacji łacińskich nazw dla nowoodkrytych gatunków roślin! Istniejące nazwy, na których nasz model się nauczy, pobierzemy ze zbioru danych UCI Plants [(TUTAJ)](https://archive.ics.uci.edu/dataset/180/plants) (<font size=2>Hamalinen,W.. (2008). Plants. UCI Machine Learning Repository. https://doi.org/10.24432/C5HS40.</font>)\n",
    "\n",
    "Na początku wykonaj poniższy fragment kodu, aby zaimportować biblioteki niezbędne do wykonania poniższego ćwiczenia:\n",
    "* **NumPy** - biblioteka umożliwiająca wykonywanie wysoko zoptymalizowanych obliczeń matematycznych na objektach typu *numpy array* (wielowymiarowych tablic),\n",
    "* **PyTorch** - biblioteka wspomagająca budowanie architektur sieci neuronowych, posiadająca wbudowane moduły odpowiadające różnym warstwom sieci neuronowych, automatyczne obliczanie gradientów (*autograd*) niezbędne do przeprowadzenia treningu sieci neuronowych,\n",
    "* **wget** - biblioteka umożliwiająca pobieranie plików z zewnętrznych źródeł (np. stron www) oraz **os** - biblioteka umożliwiająca zarządzanie tymi plikami z poziomu systemu operacyjnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T17:55:46.325764200Z",
     "start_time": "2023-11-20T17:55:42.770195500Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! python -m pip install numpy==1.22.3\n",
    "# ! python -m pip install torch==2.0.1\n",
    "# ! python -m pip install wget==3.2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wget\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie i przygotowanie danych\n",
    "\n",
    "Na wstępie musimy przygotować odpowiednio dane, na których będziemy pracowali. W pierwszej części przygotowanego przeze mnie kodu dokonuje się pobranie i wczytanie pliku `plants.data`, zawierającego dane z zestawu Plants: najpierw dane wczytywane są linijka po linijce, a następnie przekształcane do tensora składającego się z ciągu znaków (przykładowo, nazwa pierwszej rośliny, *abelia*, widoczna jest jako lista `['a','b','e','l','i','a']`; po każdej nazwie sztucznie dodaję znak nowej linii `\\n`).  \n",
    "\n",
    "Musimy pamiętać, że komputer operuje na liczbach, a nie znakach alfanumerycznych. Musimy zatem przekształcić znaki do postaci liczb. Kolejnym ważnym krokiem w przygotowaniu danych jest zatem stworzenie dwóch słowników:\n",
    "* `char_to_num`, zawierającego powiązania pomiędzy znakami a przypisanymi im liczbami-indeksami (np. `{'a':4}`)\n",
    "* `num_to_char`, zawierającego odwrotne powiązania, tj. indeks:znak (np. `{4:'a'}`). \n",
    "\n",
    "<font size=2>Aby to zrobić, wystarczy przeiterować po wszystkich elementach listy `characters`, zapamiętując analizowany element listy oraz jego indeks (w tym celu warto przekształcić listę do formy `enumerate` ([TUTAJ](https://www.geeksforgeeks.org/enumerate-in-python/))).</font>\n",
    "\n",
    "Po utworzeniu tych słowników, przeiteruj po każdym znaku ze zbioru `input_text`, aby przekonwertować dane wejściowe ze znaków na ich indeksy! Po uzupełnieniu i uruchomieniu poniższego kodu, powinieneś widzieć efekty tego przekształcenia (od teraz dla naszego modelu, *abelia* to `[4,5,8,15,12,4]`). Od razu przeksztłać otrzymaną listę w  `torch.tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T18:10:57.347800100Z",
     "start_time": "2023-11-20T18:10:28.967001800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytano tekst; przykładowy fragment: \n",
      "['a', 'b', 'e', 'l', 'i', 'a', '\\n', 'a', 'b', 'e', 'l', 'i', 'a', ' ', 'x', ' ', 'g', 'r', 'a', 'n']\n",
      "Pojedynczych znaków: 30\n",
      "Fragment tekstu po zakodowaniu: \n",
      "tensor([ 4,  5,  8, 15, 12,  4,  0,  4,  5,  8, 15, 12,  4,  1, 27,  1, 10, 21,\n",
      "         4, 17])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- Inicjalizacja -----------------------\n",
    "# Pobierz i wczytaj plik z tekstem - na razie linijka po linijce\n",
    "if not os.path.exists(\"utils/plants.data\"):\n",
    "    wget.download(\"https://raw.githubusercontent.com/Argenni/GUiAO_lab/main/utils/plants.data\", out=\"utils/plants.data\")\n",
    "with open(\"utils/plants.data\", \"r\") as f:\n",
    "    data_raw = [s.strip() for s in f.readlines()]\n",
    "# Zapisz wczytany tekst w formie pojedynczych znaków\n",
    "data_lines = []\n",
    "for data_line in data_raw: # Dla każdej wczytanej linijki tekstu:\n",
    "    data_split = data_line.split(\",\") # wyodrębnij tylko kolumnę z łacińskimi nazwami roślin\n",
    "    data_lines.append([*data_split[0]+\"\\n\"]) # rozbij tekst na pojedyncze znaki i dodaj na końcu znak końca linii \n",
    "input_text = sum(data_lines,[]) # zapisz w formie spójnej listy\n",
    "print(\"Wczytano tekst; przykładowy fragment: \")\n",
    "print(input_text[0:20])\n",
    "# Wyodrębnij pojedyncze znaki (utwórz zbiór characters ze znakami)\n",
    "characters = sorted(set(input_text))\n",
    "print(\"Pojedynczych znaków: \"+str(len(characters)))\n",
    "\n",
    "# ----------------------- UZUPEŁNIJ KOD ----------------------------------\n",
    "# Każdemu znakowi przypisz jego reprezentację numeryczną (i na odwrót) - utwórz dwa słowniki\n",
    "char_to_num = { char:index for index,char in enumerate(characters) }\n",
    "num_to_char = { index:char for index,char in enumerate(characters) }\n",
    "# Zakoduj całość wczytanego tekstu i zapisz jako torch.tensor\n",
    "embedded_text = [char_to_num[char] for index,char in enumerate(input_text)]\n",
    "embedded_text = torch.tensor(embedded_text)\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "print(\"Fragment tekstu po zakodowaniu: \")\n",
    "print(embedded_text[0:20])\n",
    "embedded_text = torch.unsqueeze(embedded_text, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architektura sieci neuronowej\n",
    "\n",
    "Czas zaprojektować architekturę naszej rekurencyjnej sieci neuronowej - stworzymy ją z wykorzystaniem  biblioteki PyTorch, w klasie o nazwie `RNN`. Tym razem nasza architektura będzie stosunkowo prosta: zbudujemy są z trzech PyTorchowych bloków.\n",
    "* Pierwszym blokiem - `self.encoder` - będzie `nn.Embedding` ([TUTAJ](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)), która dokonuje zamiany wejściowych danych (indeksów znaków) w postaci zwykłego skalara do zakodowanego wektora - przypomnijmy sobie przekształcenie *one-hot-encoding*. Musimy podać tej warstwie informacje o ilości kodowanych liczb (`num_embeddings`) oraz oczekiwanej długości wektora po zakodowaniu (`embedding_dim`) - w tym przypadku, oba te argumenty muszą być równe i wynosić ilość pojedynczych znaków występujących w naszym tekście (u nas jest to zmienna `input_size`).\n",
    "* Drugim blokiem - `self.lstm` - jest sieć LSTM `nn.LSTM` ([TUTAJ](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)) - to ją będziemy trenować. Musimy jej podać takie dane, jak: wymiarowość danych (`input_size`), wielkość *hidden state* (`hidden_size`) oraz ilość warstw, z ilu składa się nasza LSTM (`num_layers`) - tę wartość ustawmy na stałe na 3.\n",
    "* Ostatnim blokiem jest warstwa liniowa `nn.Linear` - wejściowych neuronów powinno być tyle, ile wynosi wymiarowość *hidden_state*, a wyjściowych - tyle, ile wynosi znaków w zbiorze danych (by każdy wyjściowy neuron mógł oszacować prawdopodobieństwo, że dany znak ma być tym następnym) - jest to wartość przechowywana w argumencie `output_size`.\n",
    "\n",
    "Uzupełnij zatem metody `__init__` i `forward` klasy RNN. Układając przepływ danych, pamiętaj, że sieć LSTM oprócz przyjmowania danych wejściowych `X` i zwracaniu `output`, przyjmuje też i zwraca zawartość *hidden state* - `hidden_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T18:23:31.265566400Z",
     "start_time": "2023-11-20T18:23:31.257059700Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model rekurencyjnej sieci neuronowej: Embedding -> LSTM -> Linear\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Definiuje budowę sieci. Argumenty: \\n\n",
    "        - input_size - wielkość tworzonych na pierwszej warstwie embeddingów\n",
    "            (dla OHE równe ilości znaków w słowniku), int, skalar, \\n\n",
    "        - hidden_size - długość pamiętanego hidden state, int, skalar, \\n\n",
    "        - output_size - ilość neuronów na wyjściu warstwy liniowej, też równa ilości znaków w słowniku, int, skalar.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # ---------------------- UZUPEŁNIJ KOD -----------------------------------\n",
    "        # Zacznij definiować budowę sieci:\n",
    "        # Embedding layer\n",
    "        self.encoder = torch.nn.Embedding(num_embeddings=input_size, embedding_dim=input_size)\n",
    "        # LSTM\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=3)\n",
    "        # Linear\n",
    "        self.decoder = torch.nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "        # ---------------------------------------------------------------------\n",
    "\n",
    "    def forward(self, X, hidden_state=None):\n",
    "            \"\"\" \n",
    "            Definiuje przepływ danych w sieci. Argumenty: \\n \n",
    "            - X - dane wejściowe, tj.  shape=(sequence_len, input_size), \\n\n",
    "            - hidden_state - (opcjonalnie) dane, którymi chcemy inicjalizować hidden_state naszej LSTM,\n",
    "                torch.tensor, shape=(num_layers, batch_size, hidden_size) domyślnie None, co oznacza inicjalizację zerami. \\n\n",
    "            Zwraca: output - odpowiedź sieci, torch.tensor, shape=(sequence_size, batch_size, hidden_size).\n",
    "            \"\"\"\n",
    "            # ------------------- UZUPEŁNIJ KOD ----------------\n",
    "            # Zakoduj wejściową sekwencję (OHE) z wykorzystaniem warstwy Embedding\n",
    "            X = self.encoder(X)\n",
    "            # Przekaż zakodowaną sekwencję do LSTMa (LSTM zwraca swoje wyjście oraz zawartość hidden state)\n",
    "            output, hidden_state = self.lstm(X, hidden_state)\n",
    "            # Ostateczna predykcja - wyjście warstwy Linear\n",
    "            output = self.decoder(output)\n",
    "            # --------------------------------------------------\n",
    "            return output, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generacja tekstu\n",
    "\n",
    "Ideą działania tak zdefiniowanej przez nas sieci neuronowej jest to, by na jej wyjściu otrzymywać wartości liczbowe, na podstawie których możemy określić pewien rozkład prawdopodobieństwa - każdemu znanemu znakowi przypisywane jest prawdopodobieństwo bycia następnym znakiem w generowanej sekwencji. Aby otrzymać ten rozkład, ,,znormalizujemy'' sobie wyjścia naszej sieci z pomocą funkcji `softmax`, a następnie utworzymy go, korzystając z `torch.distributions.Categorical` ([TUTAJ](https://pytorch.org/docs/stable/distributions.html#categorical)). Taki rozkład następnie próbkujemy (metodą `sample()`), aby wylosować następny znak - nie chcemy wybierać tylko znaku z najwyższym prawdopodobieństwem, bo generowane przez nas sekwencje byłyby monotonne! \n",
    "\n",
    "Napisz teraz funkcję `generate_text_embedding`, która dokonuje generacji ciągu indeksów znaków. \n",
    "\n",
    "Moje dodatkowe założenia:\n",
    "* Wygenerowany tekst ma być reakcją sieci neuronowej na znak końca linii - a zatem to, co w zbiorze danych jest początkiem kolejnej łacińskiej nazwy rośliny.\n",
    "* Wygenerowana sekwencja ma zawierać nie więcej niż `sequence_size` znaków i jednocześnie nie więcej niż 2 wygenerowane nazwy (tj. mogą wystąpić maksymalnie dwa znaki końca linii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T18:42:46.511504Z",
     "start_time": "2023-11-20T18:42:46.507505400Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_embedding(sequence_size, rnn, char_to_num):\n",
    "    \"\"\"\n",
    "    Funkcja generująca (za pomocą stworzonego przez nas RNNa klasy RNN) tekst znak po znaku, \n",
    "    w formie ciągu numerów-indeksów znaków zgodnych ze słownikiem char_to_num. Argumenty: \\n\n",
    "    - sequence_size - maksymalna długość generowanej sekwencji znaków, int, skalar, \\n\n",
    "    - rnn - obiekt klasy RNN (zdefiniowanej przez nas wcześniej), zawierający naszą RNN, \\n\n",
    "    - char_to_num - słownik, wiążący symbol znaku (np. \"a\") z przypisanym mu numerem int (np. 0), len=len(characters). \\n\n",
    "    Zwraca: output_sequence - lista zawierająca indeksy kolejnych wygenerowanych znaków (zgodne z char_to_num).\n",
    "    \"\"\"\n",
    "    rnn.eval() # przestawienie sieci neuronowej w tryb predykcji, a nie treningu\n",
    "    input_sequence = torch.tensor(char_to_num[\"\\n\"]).reshape(1,-1) # niech generowana sekwencja będzie odpowiedzą na znak nowej linii\n",
    "    output_sequence = [] # zmienna przechowująca wygenerowaną sekwencję znaków\n",
    "    hidden_state = None # inicjalizacja hidden state (tak naprawdę) samymi zerami\n",
    "    newline_counter = 0 # zliczanie ilości wygenerowanych znaków nowej linii - w celu zatrzymania generacji\n",
    "    char_counter = 0 # zliczanie ilości wygenerowanych znaków - w celu zatrzymania generacji\n",
    "    while (True):\n",
    "        # ------------------------- UZUPEŁNIJ KOD ---------------------------------\n",
    "        # Wygeneruj sekwencję znaków z wykorzystaniem naszej sieci \n",
    "        pred, hidden_state = rnn(input_sequence, hidden_state)\n",
    "        # \"Znormalizuj\" wyjście, stosując funkcję softmax\n",
    "        pred = torch.nn.functional.softmax(torch.squeeze(pred), dim=0)\n",
    "        # Określ rozkład prawdopodobieństwa następnego znaku\n",
    "        distribution = torch.distributions.Categorical(torch.squeeze(pred))\n",
    "        # Próbkuj z tego rozkładu - to będzie nowy znak\n",
    "        output = distribution.sample()\n",
    "        # Zapisz indeks nowego znaku do output_sequence\n",
    "        output_sequence.append(output.item())\n",
    "        # -------------------------------------------------------------------------\n",
    "        input_sequence[0][0]=output.item() # obecne wyjście = nowe wejście\n",
    "        char_counter = char_counter + 1 # wygenerowano nowy znak, więc zwiększ licznik o 1\n",
    "        if (output.item()==char_to_num[\"\\n\"]): newline_counter = newline_counter + 1 # jeśli wygenerowano znak nowej linii, zwiększ licznik\n",
    "        if (newline_counter>=2): break # zatrzymaj generację, jeśli wygenerowano 2 linijki tekstu...\n",
    "        if (char_counter >= sequence_size): break # lub kiedy długość sekwencji przekraczałaby sequence_size\n",
    "    return output_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inna niezbędna funkcja `embedding_to_string` ma za zadanie sekwencję znaków w postaci indeksów przekształcić do formy zwykłego tekstu (stringa). Chodzi o to, by, przykładowo, z listy `[4,5,6,7]` otrzymać zrozumiały string `abcd`.\n",
    "\n",
    "Przeiteruj zatem po wszystkich elementach pewnej sekwencji indeksów `sequence_embeddings`, odszukaj odpowiadające im znaki w słowniku `num_to_char` i zapisz w osobnej liście `gen`. W ostatniej części, zrealizowałam już łączenie takiej listy znaków w jeden spójny string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T18:42:48.087064800Z",
     "start_time": "2023-11-20T18:42:48.080558700Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding_to_string(sequence_embeddings, num_to_char):\n",
    "    \"\"\"\n",
    "    Funkcja zamieniająca wygenerowaną sekwencję w postaci listy indeksów znaków na pojedynczy string. Argumenty: \\n\n",
    "    - sequence_embeddings - lista zawierajaca indeksy wegenerowanych znaków (int) zgodne ze słownikiem num_to_char, \\n\n",
    "    - num_to_char - słownik, wiążący indeks znaku (int) (np. 0) z właściwym znakiem (np.  \"a\"), len=len(characters). \\n\n",
    "    Zwraca: text - string zawierający spójną, zdekodowaną, zrozumiałą dla człowieka wygenerowaną sekwencję znaków.\n",
    "    \"\"\"\n",
    "    # ---------------------- UZUPENIJ KOD ------------------------\n",
    "    # Utwórz listę gen, która zawiera pobrane ze słownika num_to_char znaki odpowiadające indeksom z sequence_embeddings\n",
    "    gen = [num_to_char[index] for index in sequence_embeddings]\n",
    "    # -----------------------------------------------------------\n",
    "    text = \"\" # inicjalizacja zmiennej przechowującej tekst\n",
    "    text = text.join(gen) # połącz elementy listy w jeden, spójny tekst\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kod z poniższej komórki pozwoli Ci sprawdzić poprawność implementacji powyższych funkcji. Sprawdź, czy bez zgłaszania błędów, próbny wyygenerowany tekst w formie indeksów będzie pewnymi liczbami naturalnymi, a próbny tekst zdekodowany z indeksów 4,5,6,7 to abcd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T18:43:04.551465300Z",
     "start_time": "2023-11-20T18:43:04.514283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Próbny wygenerowany tekst (bez treningu sieci) w formie indeksów znaków:\n",
      "[17, 7, 11, 23]\n",
      "Próbny tekst zdekodowany z indeksów 4,5,6,7:\n",
      "abcd\n"
     ]
    }
   ],
   "source": [
    "# Próbna generacja ciągu 4 znaków\n",
    "rnn = RNN(len(num_to_char), 512, len(num_to_char)) # obiekt z naszą siecią\n",
    "rnn = rnn.double()\n",
    "text_embedding_test = generate_text_embedding(4, rnn, char_to_num)\n",
    "print(\"Próbny wygenerowany tekst (bez treningu sieci) w formie indeksów znaków:\")\n",
    "print(text_embedding_test)\n",
    "# Próbne zdekonowanie ciągu 4 znaków - abcd\n",
    "text_test = embedding_to_string([4,5,6,7], num_to_char)\n",
    "print(\"Próbny tekst zdekodowany z indeksów 4,5,6,7:\")\n",
    "print(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening rekurencyjnej sieci neuronowej\n",
    "\n",
    "Trening naszego RNNa poprowadź podobnie, jak w przypadku dotychczasowych sieci neuronowych. Musimy przestawić sieć w tryb treningu, przeprowadzić *forward pass* z wykorzystaniem pewnej wejściowej sekwencji znaków, odebrać sekwencję wygenrowaną przez model, obliczyć wartość funkcji kosztu (użyjemy znaną Ci już CrossEntropyLoss), wyzerować dotychczasowe gradienty, przeprowadzić propagację wsteczną kosztu i zaktualizować parametry w jednym kroku algorytmu optymalizacji (znów skorzystamy z optymalizatora Adam).\n",
    "\n",
    "Jedyną różnicą jest sposób walidacji otrzymywanych z sieci wyników. Chcemy, by model umiał generować tekst podobny do treningowego - czyli w odpowiedzi na sekwencję znaków wyciętą z pewnego miejsca ze zbioru danych, powinien wygenerować sekwencję wyglądającą jak ciąg kolejnych (o 1 miejsce dalszych) znaków."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T18:43:06.011463500Z",
     "start_time": "2023-11-20T18:43:06.008339700Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_LSTM(dataset, sequence_size, num_to_char, char_to_num):\n",
    "    \"\"\"\n",
    "    Funkcja wykonująca trening naszej rekurencyjnej sieci neuronowej klasy RNN. Argumenty: \\n\n",
    "    - dataset - torch.tensor zawierający zakodowane (w formie indeksów, int) znaki tekstu treningowego, \\n\n",
    "    - sequence_size - maksymalna długość generowanej sekwencji znaków, int, skalar, \\n\n",
    "    - num_to_char - słownik, wiążący indeks znaku (int) (np. 0) z właściwym znakiem (np.  \"a\"), len=len(characters), \\n\n",
    "    - char_to_num - słownik, wiążący symbol znaku (np. \"a\") z przypisanym mu numerem int (np. 0), len=len(characters). \\n\n",
    "    Zwraca: rnn - wytrenowana sieć (obiekt klasy RNN).\n",
    "    \"\"\"\n",
    "    rnn = RNN( # tworzenie obiektu klasy RNN z hidden state o wielkości 512\n",
    "        input_size=len(num_to_char), \n",
    "        hidden_size=512, \n",
    "        output_size=len(num_to_char))\n",
    "    rnn = rnn.double()\n",
    "    # ----------------------------------- UZUPEŁNIJ KOD ----------------------------------------\n",
    "    # Utwórz obiekt optimizer, odwołujący się do algorytmu optymalizacji Adam\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=0.002)\n",
    "    # Utwórz obiekt criterion, odwołujący się do funkcji kosztu CrossEntropyLoss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(700): # trening wykonaj w 700 iteracjach\n",
    "        rnn.train() # przełączenie sieci w tryb treningu\n",
    "        start_idx = np.random.randint(dataset.shape[0]-sequence_size) # wygeneruj losowy początek sekwencji treningowej\n",
    "        # Wyodrębnij z dataset sekwencję treningową - o początku w start_idx i długości sequence_size\n",
    "        input_sequence = dataset[start_idx : start_idx+sequence_size]\n",
    "        # Wyodrębnij z dataset sekwencję \"walidacyjną\" (do liczenia kosztu) - o 1 przesuniętą w stosunku do input_sequence\n",
    "        val_sequence = dataset[start_idx+1 : start_idx+sequence_size+1]\n",
    "        hidden_state = None # inicjalizacja hidden state samymi zerami\n",
    "        # Forward pass - przekaż do sieci neuronowej input_sequence i zbierz predykcję i zawartość hidden_state\n",
    "        pred, hidden_state = rnn(input_sequence, hidden_state)\n",
    "        loss = criterion(torch.squeeze(pred), torch.squeeze(val_sequence)) # oblicz wartość kosztu dla tej iteracji\n",
    "        # Wyzeruj gradienty\n",
    "        optimizer.zero_grad()\n",
    "        # Przeprowadź propagację wsteczną kosztu\n",
    "        loss.backward()\n",
    "        # Wykonaj 1 iterację algorytmu optymalizacji\n",
    "        optimizer.step()\n",
    "        # -------------------------------------------------------------------------------------\n",
    "        if (epoch%100==0): # przy co 100 iteracji,\n",
    "            print(\"Epoch: \"+str(epoch)+\" , cumulative loss: \"+str(loss)) # wyświetl wartość kosztu \n",
    "            output_sequence = generate_text_embedding(sequence_size, rnn, char_to_num)\n",
    "            print(embedding_to_string(output_sequence, num_to_char)) # oraz przykładowy wygenerowany tekst\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uruchom teraz poniższy kod, aby dokonać treningu naszej sieci! Po każdej 100-ej iteracji, wyświetlać Ci się będzie aktualna wartość funkcji kosztu oraz przykładowa sekwencja generowanych przez RNN znaków - zobacz, jak od totalnie losowego ciągu znaków, coraz bardziej upodobniają się one do łacińskich nazw roślin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T18:51:20.255745400Z",
     "start_time": "2023-11-20T18:43:07.548462100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 , cumulative loss: tensor(3.3966, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n",
      "n\n",
      "obtcjpqyxiuirqm.lzzo. o\n",
      "Epoch: 100 , cumulative loss: tensor(2.7075, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n",
      "oollaycis\n",
      " envhtdui tnionnotallasosis rcbla kuiroa aneaenanosnoalm\n",
      "Epoch: 200 , cumulative loss: tensor(2.4782, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n",
      "clapaicus dor.pilim\n",
      "coruetruma\n",
      "Epoch: 300 , cumulative loss: tensor(2.1540, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n",
      "lskomhilis nevemi\n",
      "carifina plgatilhis\n",
      "Epoch: 400 , cumulative loss: tensor(2.0962, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n",
      "garokorifa astuaroagices\n",
      "eprhosium tentiischys\n",
      "Epoch: 500 , cumulative loss: tensor(2.0343, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n",
      "guuori camcimenmethlis\n",
      "flezyuladia\n",
      "Epoch: 600 , cumulative loss: tensor(2.1508, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n",
      "nocaethium  lephysmroceara\n",
      "glunustum cerbistifolia\n"
     ]
    }
   ],
   "source": [
    "# ------------- Trening sieci ---------------\n",
    "sequence_size = 150\n",
    "rnn = train_LSTM(\n",
    "    dataset=embedded_text, \n",
    "    sequence_size=sequence_size, \n",
    "    num_to_char=num_to_char,\n",
    "    char_to_num=char_to_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na sam koniec, wygeneruj ostateczne nazwy, jakie potrafi utworzyć nasz model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T18:51:39.052625500Z",
     "start_time": "2023-11-20T18:51:38.974260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeinepomoto mawetatus\n",
      "dybonticha cenoidis\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Predykcja ---------------\n",
    "# Generacja sekwencji w postaci embeddingów\n",
    "generated_embeddings = generate_text_embedding(sequence_size, rnn, char_to_num)\n",
    "# Zamień embeddingi na tekst\n",
    "generated_text = embedding_to_string(\n",
    "    sequence_embeddings=generated_embeddings, \n",
    "    num_to_char=num_to_char)\n",
    "# Wyświetl wygenerowany tekst\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brawo, umiesz już dokonywać generacji prostego tekstu z wykorzystaniem rekurencyjnych sieci neuronowych!\n",
    "\n",
    "\n",
    "## 5. Pytania kontrolne\n",
    "1. Na czym polega działanie rekurencyjnych sieci neuronowych?\n",
    "2. Wymień kilka rodzajów problemów, jakie można rozwiązać z wykorzystaniem RNN.\n",
    "3. Czym różni się LSTM od \"klasycznej\" sieci RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. **RNN** - sieć neuronowa, która dokonuje predykcji nie tylko na podstawie „obecnych” danych, ale też „wcześniejszych”, np. xt−1 (a dokładniej mówiąc, od pewnego ukrytego stanu ht−1 zależącego od xt−1).\n",
    "2. **Jakie problemy można rozwiązać wykorzystując RNN (przetwarzanie języka naturalnego - NLP)**:\n",
    "- tłumaczenie zdań z jednego języka w drugi,\n",
    "- uzupełnianie brakujących słów/liter w tekście, generacja tekstu,\n",
    "- rozpoznawanie mowy i tekstu (OCR),\n",
    "- streszczanie długich tekstów, wyszukiwanie sentymentu.\n",
    "- analiza sekwencji danych (i zwracać pojedynczą wartość, bądź inną sekwencję, bądź generować nowe sekwencje)\n",
    "3. **LSTM:**\n",
    "- wprowadzają mechanizm pamięci długotrwałej\n",
    "- ułatwia analizę długich sekwencji\n",
    "- umożliwia zrównoleglenie obliczeń\n",
    "- wprowadzają dodatkową pamięć ct przechowująca szerszy kontekst, tak długo jak będzie potrzebny\n",
    "- wprowadza dodatkowe bramki - bramka wejścia, zapomnienia, wyjścia. Pozwalają na kontrolowanie przepływu informacji w sieci: \n",
    "  - update gate - jak silnie opierać się na obecnych danych podczas uaktualniania hidden state\n",
    "  - forget gate - wpływają na to które dane z przeszłości zapomnieć\n",
    "  - output gate - jak bardzo opierać się na komórce ct podczas uaktualnienia ht.\n",
    "- tutaj wprowadzono koncepcję komórki przechowywującej informację z poprzednich kroków czasowych\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
