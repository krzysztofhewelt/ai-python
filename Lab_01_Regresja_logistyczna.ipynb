{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "<font size=\"5\">\n",
    "\n",
    "Laboratorium z przedmiotu: \\\n",
    "**Głębokie uczenie i analiza obrazów**\n",
    "\n",
    "Ćwiczenie 1: \\\n",
    "**Regresja logistyczna i inne klasyczne algorytmy klasyfikacji**\n",
    "\n",
    "</font>\n",
    "\n",
    "\\\n",
    "Marta Szarmach \\\n",
    "Zakład Telekomunikacji Morskiej \\\n",
    "Wydział Elektryczny \\\n",
    "Uniwersytet Morski w Gdyni\n",
    "\n",
    "07.2023\n",
    "</div>\n",
    "\n",
    "\n",
    "# 1. Wprowadzenie\n",
    "\n",
    "**Uczenie maszynowe** jest to zbiór algorytmów, które odnajdują zależności ukryte w danych, potrafią te zależności modelować (tj. opisywać za pomocą matematycznych struktur) bez bycia jawnie zaprogramowanym przez człowieka i doskonalą swoje działanie (tj. potrafią dobierać lepsze parametry opisujące modele) dzięki dostarczeniu do nich nowych danych (uczenie maszynowe jest jedynie częścią szerszego zagadnienia, jakim jest **sztuczna inteligencja**, tj. dziedzina nauki z pogranicza informatyki, matematyki, kongwinistyki i neurologii, zajmująca się tworzeniem maszyn/oprogramowania ,,udających'' ludzką inteligencję, tj. potrafiącego analizować dostarczone doń dane, wyciągać na ich podstawie wnioski i podejmować decyzje).  Innymi słowy, w przeciwieństwie do klasycznego programowania, nie tworzy się gotowych reguł, lecz algorytm sam, na podstawie dostarczonych danych i spodziewanych odpowiedzi na nie, określa reguły, tzn. tworzy **model**, który stara się odzwierciedlić strukturę danych i sposób wyznaczania tychże odpowiedzi. Model najczęściej opisany jest za pomocą fukcji (hipotezy) zależnej od **parametrów** $\\theta$, które są pewnymi wartościami liczbowymi ulegającymi zmianie w ramach treningu (nie należy mylić parametrów z **hiperparametrami**, które również opisują w pewnym sensie model, lecz nie są wyznaczane podczas treningu, a wręcz przeciwnie, muszą być podane wcześniej).\n",
    "\n",
    "Aby stworzyć i wytrenować model, najczęściej określa się pewną funkcję (nazywaną **funkcją kosztu** $J(\\theta)$), która jest w stanie policzyć, jak bardzo model myli się podczas dokonywania predykcji. **Trening** polega na optymalizacji funkcji kosztu, a dokładniej mówiąc, iteracyjnej aktualizacji parametrów modelu, aż koszt na danych treningowych staje się (najczęściej) minimalny - wówczas uważa się, że model możliwie najlepiej odzwierciedla te dane. Jedną z metod optymalizacji jest **metoda gradientu prostego**, której działanie opiera się na poszukiwaniu lokalnego minimum poprzez wyznaczanie gradientu $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ (kierunku najszybszej zmiany) funkcji kosztu i aktualizacji parametrów zgodnie z tym gradientem. \n",
    "\n",
    "\\\n",
    "Uczenie maszynowe można podzielić na dwie podstawowe grupy: \n",
    "* **uczenie nadzorowane**, w którym każdy rekord danych dostarczony do algorytmu posiada **etykietę** zawierającą pożądaną odpowiedź algorytmu na ten konkretny rekord. Algorytmy uczenia nadzorowanego potrafią rozwiązywać takie problemy jak:\n",
    "    * **regresja** - przypisanie do rekordu danych pewnej dowolnej liczby,\n",
    "    * **klasyfikacja** -  przypisanie do rekordu danych liczby (z przedziału dyskretnego), symbolizującej jego przynależność do pewnej klasy,\n",
    "* **uczenie nienadzorowane** - dane nie posiadają predefiniowanych etykiet, algorytmy muszą same znaleźć strukturę w danych. Rozwiązują takie problemy, jak m.in.:\n",
    "    * **grupowanie** - podział zebranych danych na grupy, tak, aby dane z jednej grupy były bardziej podobne do siebie niż do danych z innych grup,\n",
    "    * **wykrywanie anomalii** - odnalezienie w zbiorze danych tych rekordów, które w pewien sposób odróżniają się od reszty.\n",
    "\n",
    "W niniejszym ćwiczeniu zajmować się będziemy jedynie zagadnieniem klasyfikacji. Niektórymi algorytmami uczenia maszynowego realizującymi klasyfikację są:\n",
    "* **Maszyna wektrów wspierających** (ang. *Support Vector Machine*, **SVM**) - algorytm, który dokonuje klasyfikacji danych poprzez utworzenie (z pomocą dodatkowych funkcji, tzw. kerneli) dodatkowego wymiaru i hiperpłaszczyzny, która oddziela na tym wymiarze dane z różnych klas z maksymalnym możliwym marginesem,\n",
    "* **Drzewo decyzyjne** (ang. *Decision Tree*) - zbiór hierarchicznie następujących po sobie instrukcji warunkowych, których ostatnia warstwa decyduje o wyniku predykcji,\n",
    "* *k* **najbliższych sąsiadów** (ang. *k Nearest Neighbours*, *k*-NN) - to, jaka zostanie podjęta decyzja dotycząca badanego rekordu, zależy od etykiet $k$ innych rekordów najbliższych temu rekordowi,\n",
    "* a także **regresja logistyczna** - która to będzie głównym zagadnieniem niniejszego ćwiczenia. W toku działania tegoż algorytmu, dokonuje się dopasowania pewnej funkcji, wiążącej parametry $\\theta$ i dane X, której zadaniem jest oszacowanie prawdopodobieństwa, z jakim określony rekord danych należy do pewnej klasy.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:828/1*PQ8tdohapfm-YHlrRIRuOA.gif' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cel ćwiczenia\n",
    "\n",
    "**Celem niniejszego ćwiczenia** jest zapoznanie się z działaniem klasycznych algorytmów uczenia maszynowego realizujących zagadnienia klasyfikacji poprzez:\n",
    "* implementacji ,,od zera'' algorytmu regresji logistycznej (łącznie z optymalizacją funkcji kosztu metodą gradientu prostego),\n",
    "* użycie gotowych klas z biblioteki Scikit-learn z zaimplementowanymi gotowymi klasyfikatorami (regresją logistyczną, drzewem decyzyjnym i *k*-NN) i porównanie otrzymanych wyników.\n",
    "\n",
    "\n",
    "# 3. Stanowisko laboratoryjne\n",
    "\n",
    "Do wykonania niniejszego ćwiczenia niezbędne jest stanowisko laboratoryjne, składające się z komputera klasy PC z zainstalowanym oprogramowaniem:\n",
    "* językiem programowania Python (w wersji 3.8),\n",
    "* IDE obsługującym pliki Jupyter Notebook (np. Visual Studio Code z rozszerzeniem ipykernel).\n",
    "\n",
    "\n",
    "# 4. Przebieg ćwiczenia\n",
    "## 4.1. Implementacja algorytmu regresji logistycznej ,,od zera''\n",
    "\n",
    "### Inicjalizacja: import niezbędnych elementów\n",
    "\n",
    "Na początku wykonaj poniższy fragment kodu, aby zaimportować biblioteki niezbędne do wykonania poniższego ćwiczenia:\n",
    "* **Scikit-learn** - biblioteka zawierająca gotowe implementacje wielu algorytmów klasycznego uczenia maszynowego, a także zbiory danych czy metryki. Tutaj skorzystamy ze zbioru danych iris - `datasets.load_iris`.\n",
    "* **NumPy** - biblioteka umożliwiająca wykonywanie wysoko zoptymalizowanych obliczeń matematycznych na objektach typu *numpy array* (wielowymiarowych tablic).\n",
    "* **Matplotlib** - biblioteka wspomagająca wizualizację pracy czy analizę danych poprzez wyświetlanie wykresów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m pip install numpy==1.22.3\n",
    "#! python -m pip install scikit-learn==0.24.2\n",
    "#! python -m pip install matplotlib==3.4.2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zaimportowaniu niezbędnych bibliotek, załadujmy gotowy zbiór danych Iris (zawierający dane o wymiarach różnych rodzajów irysów), pochodzący z repozytorium UCI (więcej informacji o tym zbiorze danych możesz uzyskać [TUTAJ](https://archive.ics.uci.edu/dataset/53/iris)). Dane zapiszmy pod zmienną X, a odpowiadające im etykiety - y.\n",
    "\n",
    "Ponadto, dla zobrazowania danych, które będziemy używać, wyświetlmy rozmiary tablic X i y, a także 5 pierszwych rekordów/etykiet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[0:100,:]\n",
    "y = iris.target[0:100]\n",
    "print(\"Wymiary danych wejściowych: \" + str(X.shape))\n",
    "print(\"Przykładowe dane wejściowe: \")\n",
    "print(X[0:5,:])\n",
    "print(\"Wymiary etykiet: \" + str(y.shape))\n",
    "print(\"Przykładowe etykiety: \")\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się wyświetlonym informacjom na temat wczytanych danych. Wczytujemy 100 rekordów o 4 cechach (wymiary tablicy X to 100x4). Etykiety zapisane są w formie 1-wymiarowego wektora y (o wymiarze 100) - z dokumentacji możemy się dowiedzieć, że mogą one przyjmować jedną z 3 wartości: 0, 1 lub 2, symbolizujących przynależność rekordu do jednej z 3 klas (każda klasa oznacza inny typ irysa), lecz w naszym przypadku, aby skupić się jedynie na zagadnieniu binarnej klasyfikacji, podczas ładowania danych odrzucamy ostatnich 50 rekordów należących do 3 klasy, więc operujemy jedynie na 2 klasach.\n",
    "\n",
    "<font size=\"2\">Informacje o wymiarach otrzymanych tablic będą nam bardzo potrzebne na późniejszym etapie, podczas pracy na macierzach przy implementacji procesu terningu i predykcji naszego algorytmu.</font>\n",
    "\n",
    "\n",
    "### Przygotowanie danych\n",
    "\n",
    "Aby móc w pełni korzystać z tych danych, musimy je jednak nieco przekształcić. W tym celu wykonamy dwie operacje:\n",
    "* dokonamy **standaryzacji** danych, tj. przekształcimy je tak, aby bez zmiany ich struktury, każda z cech posiadała średnią o wartości 0 i wariancję o wartości 1 - poprawi to działanie algorytmów uczenia maszynowego, zwłaszcza podczas pracy nad danymi o szerokiej dynamice (różnych rzędach wielkości) czy różnych jednostkach,\n",
    "* podzielimy cały dostępny zbiór danych na 2 zestawy: \n",
    "    * **treningowy** - na podstawie którego model zostanie wytrenowany (to pod te dane zostaną dopasowane parametry naszego modelu),\n",
    "    * **testowy** - który posłuży nam do określenia, jak dobrze działa nasz model na danych, których wcześniej model nie widział.\n",
    "\n",
    "<font size=\"2\">W tym konkretnym przypadku nie będziemy jeszcze wydzielać trzeciego z zazwyczaj tworzonych zestawów danych, zestawu **walidacyjnego**, na podstawie którego dopasowuje się hiperparametry modelu zanim przejdzie się do jego właściwego uczenia - nie ma takiej potrzeby, gdyż nie będziemy w ramach tego ćwiczenia zajmować się dopasowaniem żadnych hiperparametrów.</font>\n",
    "\n",
    "Zacznijmy od napisania funkcji `standarize_data` służącej do normalizacji danych. Zgodnie z poniższym wzorem, zaimplementuj funkcję, która przyjmuje jako argument wejściowe (tablicę X), oblicza średnią $\\mu_n$ i odchylenie standardowe $\\sigma_n$ każdej z cech $x_n$, a następnie odejmuje od nich wyliczone średnie i dzieli je przez odchylenia stardardowe: \n",
    "\\begin{equation*}\n",
    "\tx_n = \\frac{x_n - \\mu_n}{\\sigma_n}\n",
    "\\end{equation*}\n",
    "po czym zwraca tak znormalizowane dane jako tablicę Xnorm.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `standarize_data`:\n",
    "* Zapoznaj się z dokumentacją dwóch funkcji z biblioteki NumPy: `np.mean` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)) oraz `np.std` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.std.html)). Zwróć uwagę na parametr `axis`.\n",
    "* Zastosuj mechanizm broadcastigu występujący podczas operacji na tablicach z użyciem NumPy (więcej o tym możesz przeczytać [TUTAJ](https://numpy.org/doc/stable/user/basics.broadcasting.html)).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_data(X):\n",
    "    \"\"\"Funkcja realizująca standaryzację danych: przyrównanie średniej każdej z cech do 0, \n",
    "    a wariancji do 1. \\n\n",
    "    Argument: X - nieprzekształcone dane (numpy array, shape = (num_samples, num_features) ). \\n\n",
    "    Zwraca: \\n\n",
    "    - Xnorm - znormalizowane dane (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    - mu - wektor ze średnimi każdej z cech (numpy array, shape = (num_features,) ), \\n\n",
    "    - sigma - wektor z odchyleniami standardowymi każdej z cech (numpy array, shape = (num_features,) ).\"\"\"\n",
    "\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz mu - średnią każdej z cech (pamiętaj, że cechy przechowywane są w każdej kolumnie tablicy X)\n",
    "    mu = np.mean(X, axis=0)\n",
    "    # Oblicz sigma - odchylenie standardowe każdej z cech\n",
    "    sigma = np.std(X, axis=0)\n",
    "    # Oblicz Xnorm - odejmij średnią od każdej z cech i podziel ją przez jej odchylenie standardowe \n",
    "    # (możesz to zrobić w 1 linijce kodu)\n",
    "    Xnorm = (X - mu) / sigma\n",
    "    # -------------------------------\n",
    "    \n",
    "    return Xnorm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz przejdźmy do utworzenia funkcji `split_data`, która ma za zadanie podzielić dostarczony jej zbiór danych X (i etykiety y) na zestaw treningowy i testowy, zgodnie z podanym jako argument procentem (wartość `percentage_train` odpowiadać ma procentowi, jaką częścią oryginalnego zbioru danych ma być zbiór treningowy).\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `split_data`:\n",
    "* Zapoznaj się z dokumentacją funkcji `np.random.choice` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)). Zwróć uwagę na parametr `replace`.\n",
    "* Zapoznaj się z trickiem na usunięcie elementów jednej listy z drugiej z wykorzystaniem różnicy zbiorów (więcej o tym możesz przeczytać [TUTAJ](https://stackoverflow.com/questions/3428536/how-do-i-subtract-one-list-from-another/)). Do utworzenia listy indeksów wszystkich rekordów danych możesz użyć funkcji `np.arange` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.arange.html)).\n",
    "* Zapoznaj się ze sposobami na wyodrębnienie części tablicy w NumPy ([TUTAJ](https://numpy.org/doc/stable/user/basics.indexing.html)). Możesz je filtrować według elementów innej listy!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y, percentage_train = 70.0):\n",
    "    \"\"\"Funkcja dzieląca losowo dane na zestaw treningowy oraz testowy w zadanej proporcji. \\n\n",
    "    Argumenty: \\n\n",
    "    - X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    - y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    - percentage_train (argument opcjonalny) - jakim procentem wejściowych danych ma być \n",
    "      zestaw treningowy (skalar, float, domyślna wartość: 70). \\n\n",
    "    Zwraca: \\n\n",
    "    - Xtrain - dane treningowe (numpy array, shape = (num_samples * percentage_train, num_features) ), \\n\n",
    "    - ytrain - etykiety do danych treningowych (numpy array, shape = (num_samples * percentage_train,) ), \\n\n",
    "    - Xtest - dane testowe (numpy array, shape = (num_samples * (100-percentage_train), num_features) ), \\n\n",
    "    - ytest - etykiety do danych testowych (numpy array, shape = (num_samples * (100-percentage_train),) ).\"\"\"\n",
    "    \n",
    "    np.random.seed(100) # Dla zapanowania nad \"losowością\"\n",
    "    num_all_datapoints =  X.shape[0] # Ilość wszystkich danych\n",
    "    num_train_datapoints = int(np.round(X.shape[0]*percentage_train/100)) # Docelowa wielkość zestawu treningowego\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Znając rozmiar danych wejściowych, wygeneruj indices_train - listę indeksów tych rekordów, \n",
    "    # które mają należeć do zestawu treningowego\n",
    "    indices_train = np.random.choice(num_all_datapoints, num_train_datapoints, replace=False)\n",
    "    # Wszystkie indeksy, które nie należą do indices_train, zapisz do indices_test\n",
    "    indices_test = list(set(np.arange(X.shape[0])) - set(indices_train))\n",
    "    # Wygeneruj zmienne z właściwie podzielonym zbiorem danych: Xtrain, ytrain, Xtest, ytest\n",
    "    Xtrain = X[indices_train,:]\n",
    "    ytrain = y[indices_train]\n",
    "    Xtest = X[indices_test,:]\n",
    "    ytest = y[indices_test]\n",
    "    # ------------------------------\n",
    "    \n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pora sprawdzić, jak działają napisane przez Ciebie funkcje! Uruchom poniższy kod, aby podzielić nasz zbiór danych na zestaw treningowy (powinien domyślnie zawierać 70 elementów) i testowy (pozostałe 30 elementów). Elementy w poszczególnych cechach powinny po normalizacji oscylować wokół 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział danych na część treningową i testową\n",
    "Xtrain, ytrain, Xtest, ytest = split_data(X,y)\n",
    "print(\"Wymiary danych treningowych: \"+str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \"+str(Xtest.shape))\n",
    "\n",
    "# Normalizacja obu zestawów danych\n",
    "Xtrain_norm, _, _ = standarize_data(Xtrain) # pomijamy zwracanie mu i sigma dla danych treningowych\n",
    "Xtest_norm, mu_test, sigma_test = standarize_data(Xtest)\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening modelu regresji logistycznej\n",
    "\n",
    "Teraz, kiedy mamy już gotowe dane, na których możemy pracować, przejdźmy do najważniejszej rzeczy, czyli napisania funkcji, które utworzą nasz model oparty na regresji logistycznej i pozwolą mu się uczyć!\n",
    "\n",
    "Przypomnijmy, że **hipotezą** $h_\\theta(x)$ (tj. funkcją, która wiąże parametry modelu i dane wejściowe, dając w wyniku predykcje) regresji logistycznej jest: \n",
    "\\begin{equation*}\n",
    "    h_\\theta(x) = g(\\theta^Tx)\n",
    "\\end{equation*}\n",
    "gdzie funkcja $g(z)$ jest to tzw. funkcja **sigmoid** o następującej postaci:\n",
    "\\begin{equation*}\n",
    "    g(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation*}\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1280/1*OUOB_YF41M-O4GgZH_F2rw.png' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>\n",
    "\n",
    "Funkcja sigmoid ma kilka ciekawych właściwości, dzięki którym jest często spotykana przy rozwiązywaniu problemów klasyfikacji: jej wartości zawierają się w przedziale od 0 do 1 (dlatego można je traktować jak prawdopodobieństwo należenia danego rekordu danych do pewnej klasy i np. traktować te dane, dla których sigmoid zwrócił wartość większą niż 0,5, jako należące do tejże klasy), a także jest odwracalna i różniczkowalna, dzięki czemu da się obliczać jej gradient niezbędny w procesie uczenia modelu. Zaimplementuj zatem funkcję `sigmoid`, która zwraca wartość sigmoidu dla dowolnej *numpy array*!\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `sigmoid`: Warto skorzystać z funkcji `np.exp` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.exp.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Funkcja obliczająca wartość sigmoidu dla zadanego argumentu z. \\n\n",
    "    \"\"\"\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    # ------------------------------\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `sigmoid`. Obliczymy, co zwraca ta funkcja, gdy argumentem są same zera: skalar, wektor 1-D oraz macierz 2-D. W każdym przypadku, sigmoid powinien zwrócić stukturę o takich samych wymiarach, jak dane wejściowe, a każdy z jej elementów powinien wynosić 0,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testowanie poprawności implementacji sigmoidu\n",
    "print(\"Sigmoid dla skalara: \" + str(sigmoid(0)))\n",
    "print(\"Sigmoid dla wektora: \" + str(sigmoid(np.zeros((3)))))\n",
    "print(\"Sigmoid dla macierzy: \" + str(sigmoid(np.zeros((3,3)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako funkcję kosztu $J(\\theta)$ użyjemy **binarnej entropii krzyżowej** (ang. *Binary Cross Entropy*, BCE), często spotykaną przy okazji problemów klasyfikacji binarnej. Jej wartość jest tym większa, im więcej pomyłek popełni klasyfikator: przy zgodności etykiety $y^{(i)}$ i predykcji $h_\\theta(x^{(i)})$, oba człony wyrażenia zerują się. Funkcja ta opisana jest wzorem:\n",
    "\\begin{equation*}\n",
    "\tJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)}\\log(h_\\theta(x^{(i)})) - (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]\t\n",
    "\\end{equation*}\n",
    "\n",
    "Jak już wiesz, trening modelu opiera się na znalezieniu optymalnych parametrów $\\theta$, tj. takich, przy których funkcja kosztu jest minimalna. My taką optymalizację przeprowadzimy z wykorzystaniem metody gradientu prostego, która do poprawnego działania musi znać gradient $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ naszej funcji kosztu. Pamiętaj, że **gradient ma taki sam wymiar, jak wektor parametrów $\\theta$**, a zatem składa się z $n$ elementów. W przypadku gradientu fukcji BCE, każdy z jego $n$ elementów można obliczyć z następującego wzoru (pomijam tutaj jego wyprowadzenie):\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial J(\\theta)}{\\partial \\theta_n} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)}) \\cdot x^{(i)}_n\n",
    "\\end{equation*}\n",
    "\n",
    "Napisz zatem funkcję `compute_cost_and_gradient`, w której na podstawie danych treningowych i zadanych parametrów $\\theta$, obliczysz koszt BCE i jego gradient.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `compute_cost_and_gradient`:\n",
    "* Przy obliczaniu kosztu, będzie Ci na pewno potrzebna funkcja `np.log2` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.log2.html)) oraz napisana przez Ciebie wcześniej funkcja `sigmoid`.\n",
    "* Zamiast używania pętli `for` do iteracji po wszystkich $m$ elementach, możesz wykonać operacje na macierzach (z wykorzystaniem funkcji `np.dot` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.dot.html))). Pamiętaj jednak o właściwościach mnożenia macierzy - nie każde macierze da się przemnożyć, muszą one mieć zgodne \"wewnętrze\" wymiary (ilość kolumn pierwszej macierzy musi być taka sama, jak ilość wierszy drugiej macierzy; wówczas w wyniku mnożenia otrzymujemy macierz o zgodnych \"zewnętrznych\" wymiarach, tj. o liczbie wierszy jak pierwsza macierz i liczbie kolumn jak druga macierz: [M,N]x[N,1]=[M,1]): w razie potrzeby zmień kolejność mnożonych macierzy albo dokonaj ich transpozycji z użyciem funkcji `np.transpose` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)).\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T18:52:59.734050100Z",
     "start_time": "2023-10-14T18:52:59.726541600Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_cost_and_gradient(X, y, theta):\n",
    "    \"\"\"Funkcja obliczająca koszt BCE dla regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    theta - zestaw parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: \\n\n",
    "    J - obliczony koszt (skalar, float), \\n\n",
    "    grad - obliczony gradient funkcji kosztu (numpy array, shape=(num_features) ). \"\"\"\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz sigmoid\n",
    "    sig = sigmoid(np.dot(X, np.transpose(theta)))\n",
    "    \n",
    "    # Oblicz wartość funkcji kosztu\n",
    "    J = (-(np.dot(y, np.log2(sig))\n",
    "          + np.dot((1-y), np.log2(1-sig))) \n",
    "         / X.shape[0])\n",
    "    \n",
    "    # Oblicz jej gradient\n",
    "    grad = np.dot(np.transpose(X), sig - y) / X.shape[0]\n",
    "    # ------------------------------\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy poprawność implementacji powyższej funkcji: uruchom następującą komórkę, aby wyliczyć przykładowe wartości kosztu i jego gradientu dla zerowych parametrów (kiedy wszystkie elementy $\\theta$ są równe 0), liczone dla całego naszego zbioru danych. Koszt powinien wynosić 1, a gradient [-0.2325,  0.1645, -0.6995, -0.27]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T18:55:00.481960900Z",
     "start_time": "2023-10-14T18:55:00.475451300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowy koszt i jego gradient, liczony dla zerowych parametrów: (1.0, array([-0.2325,  0.1645, -0.6995, -0.27  ]))\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji funkcji kosztu\n",
    "print(\"Przykładowy koszt i jego gradient, liczony dla zerowych parametrów: \"+str(compute_cost_and_gradient(X,y,np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając wyznaczoną funkcję kosztu, możemy przejść do jej optymalizacji, czyli doboru takich wartości parametrów $\\theta$, które dają najniższą wartość kosztu (tj. najlepiej odwzorowują dane treningowe). Jak już wspomniano, zrobimy to z wykorzystaniem metody gradientu prostego, według której aktualizacja parametrów odbywa się według poniższego wzoru:\n",
    "\\begin{equation*}\n",
    "    \\theta_n := \\theta_n - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_n}\n",
    "\\end{equation*}\n",
    "gdzie hiperparametr $\\alpha$ (którego wartość musi być zdefiniowana zanim przejdzie się do treningu modelu), oznacza, jak bardzo gradient funkcji kosztu ma wpływ na nową, zaktualizowaną postać parametrów $\\theta$\n",
    "\n",
    "Napisz zatem funkcję `train_logistic_regression`, w której na podstawie danych treningowych, iteracyjnie liczona jest wartość funkcji kosztu i jego gradient, parametry są aktualizowane zgodnie z metodą gradientu prostego, a ponadto przy każdej itaracji wizualizowana jest nowa wartość kosztu, aby móc ocenić, czy w ramach treningu koszt rzeczywiście spada (ważne - jeśli zaobserwowalibyśmy wzrost kosztu wraz z kolejnymi  iteracjami, oznacza to, że model CORAZ GORZEJ radzi sobie z analizą danych treningowych, a zatem wcale się nie uczy!). \"Szkielet\" tej funkcji został już napisany, wykonanych zostanie 400 iteracji, a stała uczenia może zostać w postaci domyślnej (ustawionej na 0,01).\n",
    "\n",
    "<font size=\"2\">Poprawność implementacji tej funkcji sprawdzimy nieco później.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X,y,alpha=0.01):\n",
    "    \"\"\"Funkcja realizująca optymalizację funkcji kosztu dla regresji logistycznej\n",
    "    w celu wytrenowania modelu (otrzymania zestawu najlepszych parametrów, theta)\n",
    "    z wykorzystaniem metody gradientu prostego. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane treningowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    alpha (opcjonalnie) - stała uczenia (skalar, float, domyślnie 0.001). \\n\n",
    "    Zwraca: theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \"\"\"\n",
    "    \n",
    "    # Inicjalizacja\n",
    "    num_iterations = 400 # tyle razy wykonać ma się gradient descent\n",
    "    theta = np.zeros((X.shape[1])) # wstępna inicjalizacja parametrów samymi zerami\n",
    "    Js = np.zeros(num_iterations) # wektor przechowujący dotychczasowe wartości kosztu (do wizualizacji)\n",
    "    \n",
    "    # Uruchomienie metody gradientów prostych\n",
    "    print(\"\\nTrwa trening modelu... \")\n",
    "    for i in range(num_iterations):\n",
    "        # ------- UZUPEŁNIJ KOD --------\n",
    "        # Korzystając z wcześniej napisanej funkcji, oblicz funkcję kosztu J i jej gradient grad\n",
    "        J, grad = compute_cost_and_gradient(X,y,theta)\n",
    "        # Zapisz obliczony koszt jako odpowiedni element w Js\n",
    "        Js[i] = J\n",
    "        # Dokonaj aktualizacji parametrów theta o wcześniej obliczony gradient przemnożony przez stałą uczenia\n",
    "        theta = theta - alpha * grad\n",
    "        # ------------------------------\n",
    "    print(\"Zakończono. \")\n",
    "    \n",
    "    # Wizualizacja zmian kosztu\n",
    "    plt.figure()\n",
    "    plt.plot(Js)\n",
    "    plt.title(\"Efekty treningu modelu - zmiany w koszcie\")\n",
    "    plt.xlabel(\"Numer iteracji\")\n",
    "    plt.ylabel(\"Wartość funkcji kosztu\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predykcja i poskładanie wszystkiego w całość!\n",
    "\n",
    "Wytrenowany model musi umieć dokonywać predykcji - w tym przypadku, podejmować decyzję, czy analizowany rekord danych zaklasyfikować do klasy pierwszej (etykieta 0) czy drugiej (etykieta 1). Napisz zatem ostatnią w tej części ćwiczenia funkcję, `predict_logistic_regression`, która oblicza funkcję hipotezy dla regresji logistycznej i zwraca odpowiednie etykiety.\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `predict_logistic_regression`: Pamiętaj, że model ma zwrócić etykietę 0 w sytuacji, kiedy prawdopodobieństwo obliczone z wykorzystaniem hipotezy jest mniejsze niż 0,5. Poszukaj funkcji z biblioteki NumPy, która realizuje przybliżanie do najbliższej liczby naturalnej!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic_regression(X, theta):\n",
    "    \"\"\"Funkcja obliczająca ostateczną predykcję regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: pred - dokonane predykcje (numpy array, shape = (num_samples,) ). \"\"\"\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    pred = np.round(sigmoid(np.dot(X, np.transpose(theta))))\n",
    "    # ------------------------------\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `predict_logistic_regression`. W poniższej komórce wykonujemu tę funkcję dla pierwszych pięciu rekordów oryginalnego zbioru danych i zerowych parametrów. Powinieneś otrzymać w wyniku same zera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testowanie poprawności implementacji predykcji\n",
    "print(\"Przykładowe predykcje przy zerowych parametrach: \"+str(predict_logistic_regression(X[0:5,:],np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas uruchomić całość - od właściwego uczenia naszego modelu, aż po dokonanie przezeń predykcji na danych testowych! Uruchom poniższy kod, w którym wykorzystujemy niemal wszystko, co do tej pory udało nam się napisać. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trening modelu (na danych treningowych)\n",
    "theta = train_logistic_regression(Xtrain_norm,ytrain)\n",
    "# Dokonanie predykcji (na danych testowych) i obliczenie dokładności modelu\n",
    "pred = predict_logistic_regression(Xtest_norm,theta)\n",
    "accuracy = np.mean(pred==ytest)\n",
    "print(\"Dokładność modelu na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli na wykresie obserwujesz stopniowy spadek kosztu - świetnie, Twój model się uczy! Można też zaobserwować bardzo wysoką wartość dokładności (**dokładność** to procent decyzji, które model podjął właściwie) naszego modelu na danych z zestawu testowego, świadczącą o jego poprawnym działaniu. \n",
    "\n",
    "\n",
    "## 4.2. Uruchomienie klasyfikatorów z biblioteki Scikit-learn\n",
    "\n",
    "### Import nowych niezbędnych klas\n",
    "\n",
    "Udało Ci się stworzyć model klasyfikatora opartego o regresję logistyczną od zera. Na szczęście nie zawsze trzeba włożyć tyle pracy, aby móc używać algorytmów uczenia maszynowego. Istnieją biblioteki posiadające gotowe implementacje wielu z nich. Jedną z takich bibliotek jest wykorzystywana już przez Ciebie biblioteka Scikit-learn. Tym razem do sprawdzenia, jak z analizowanym tutaj zbiorze danych iris radzą sobie inne algorytmy, wykorzystamy jej gotowe klasy i metody:\n",
    "- model regresji logistycznej - `linear_model.LogisticRegression` (dokumentacja [TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)), \n",
    "- model drzewa decyzyjnego - `tree.DecisionTreeClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)),\n",
    "- model *k*-NN - `neighbors.KNeighborsClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)),\n",
    "- metrykę dokładności - `metrics.accuracy_score` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)),\n",
    "- klasę standaryzującą dane - `preprocessing.StandardScaler` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)),\n",
    "- funkcję dzielącą dane na zestaw treningowy i testowy - `model_selection.train_test_split` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "\n",
    "Uruchom poniższą komórkę, aby zaimportować niezbędne klasy i metody. Zapoznaj się też z ich dokumentacją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych z użyciem gotowych metod\n",
    "\n",
    "Mając wciąż w pamięci przechowywane zmienne X i y z oryginalnym, jeszcze nieznormalizowanym ani niepodzielonym zbiorem danych iris (oraz etykietami), możemy je ponownie wykorzystać w tej części ćwiczenia. Napisz więc fragment kodu, który:\n",
    "* dzieli dane X i y na właściwe zestawy Xtrain i Xtest (użyj metody `train_test_split`) - niech zestaw treningowy zawiera 70% danych,\n",
    "* standaryzuje Xtrain i Xtest (zwróć uwagę na metodę `fit_transform` klasy `StandardScaler`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T18:57:02.872247800Z",
     "start_time": "2023-10-14T18:57:02.864229600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych treningowych: (70, 4)\n",
      "Wymiary danych testowych: (30, 4)\n",
      "Przykładowe znormalizowane dane treningowe: \n",
      "[[-0.03786943 -0.96674966  1.02983717  0.74849749]\n",
      " [ 0.27399646  2.58457562 -0.93683212 -0.69191005]\n",
      " [-1.44126595  0.01972958 -0.93683212 -1.05201194]\n",
      " [-1.12940005 -0.17756626 -1.0046483  -1.23206288]\n",
      " [ 2.14519182  0.01972958  1.36891808  1.28865032]]\n"
     ]
    }
   ],
   "source": [
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Podziel dane z X i y na zestaw treningowy (70%) i testowy (30%) z wykorzystaniem metody train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)\n",
    "# Dokonaj normalizacji danych treningowych z wykorzystaniem klasy StandardScaler - zwróć Xtrain_norm\n",
    "scaler = StandardScaler()\n",
    "Xtrain_norm = scaler.fit_transform(Xtrain)\n",
    "# Dokonaj normalizacji danych testowych - zwróć Xtest_norm\n",
    "Xtest_norm = scaler.fit_transform(Xtest)\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Wymiary danych treningowych: \"+str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \"+str(Xtest.shape))\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresja logistyczna z biblioteki Scikit-learn\n",
    "\n",
    "Mamy przygotowane dane, pora zatem wytrenować model regresji logistycznej z wykorzystaniem klasy `LogisticRegression`!\n",
    "* Do trenowania (na bazie zestandaryzowanego zestawu treningowego) użyj metody `fit`.\n",
    "* Do dokonania predykcji użyj metody `predict` - zobacz, jakich predykcji dokona Twój model na widok danych z zestandaryzowanego zestawu testowego (zapisz je do zmiennej o nazwie `pred`).\n",
    "* Oblicz dokładność predykcji Twojego modelu (dla zestandaryzowanych danych testowych) z wykorzystaniem metody `accuracy_score`. Wynik zapisz do zmiennej o nazwie `acccuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T18:57:17.035520400Z",
     "start_time": "2023-10-14T18:57:17.029013600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność modelu na danych testowych: 100.0%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression() # zostawmy domyślne ustawienia\n",
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Wytrenuj model regresji logistycznej na zestandaryzowanych danych treningowych\n",
    "model.fit(Xtrain_norm, ytrain)\n",
    "# Zwróć predykcję modelu (do zmiennej o nazwie pred) dla  zestandaryzowanych danych testowych\n",
    "pred = model.predict(Xtest_norm)\n",
    "# Oblicz dokładność predykcji\n",
    "accuracy = accuracy_score(ytest, pred)\n",
    "# ----------------------------------------\n",
    "print(\"Dokładność modelu na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T18:57:18.702380700Z",
     "start_time": "2023-10-14T18:57:18.696262100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność drzewa decyzyjnego na danych testowych: 100.0%\n",
      "Dokładność k-NN na danych testowych: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krzysiu\\anaconda3\\envs\\ai_3_8\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Drzewo decyzyjne\n",
    "model2 = DecisionTreeClassifier(max_depth=5)\n",
    "model2.fit(Xtrain_norm,ytrain)\n",
    "pred = model2.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność drzewa decyzyjnego na danych testowych: \"+str(accuracy*100)+'%')\n",
    "\n",
    "# k-NN\n",
    "model3 = KNeighborsClassifier(n_neighbors=3)\n",
    "model3.fit(Xtrain_norm,ytrain)\n",
    "pred = model3.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność k-NN na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inne klasyfikatory dostępne w Scikit-learn\n",
    "\n",
    "Na sam koniec, sprawdź, jak zachowują się inne modele klasyfikatorów, dostępne w ramach biblioteki Scikit-learn: drzewo decyzyjne oraz *k*-NN! Uruchom po prostu poniższy kod:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gratulacje! W ten sposób zakończyliśmy to ćwiczenie, w którym skupiliśmy się na działaniu klasycznych algorytmów klasyfikacji.\n",
    "\n",
    "\n",
    "## 5. Pytania kontrolne\n",
    "\n",
    "1. Czym różni się uczenie nadzorowane od nienadzorowanego?\n",
    "2. Jakie są różnice pomiędzy problemem regresji a klasyfikacji?\n",
    "3. Opisz krótko, na czym polega trening modelu uczenia maszynowego z wykorzystaniem metody gradientów prostych."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. **Uczenie nadzorowane** - do każdego rekordu dostarczamy etykietę (odpowiedź, czyli dane wyjściowe). Tutaj celem jest nauczenie modelu przewidywania wyjść na podstawie nowych danych wejściowych. Rozwiązuje problemy regresji i klasyfikacji.\n",
    "   **Uczenie nienadzorowane** - do danych nie dostarczamy etykiet, algorytm sam ma znaleźć odpowiedź. Model ma odnaleźć nowe wzorce, struktury, odpowiedzi. Rozwiązuje problemy grupowania, redukcji wymiarów, asocjacje.\n",
    "2. **regresja** - przewidywanie wartości liczbowej. Wynik jest punktem na osi liczbowej. Równanie matematyczne.\n",
    "   **klasyfikacja** - przyporządkowanie rekordu do danej grupy, symbolizuje przynależność do jakieś klasy. Może być binarna (0, 1), wieloklasowa, wieloetykietowa.\n",
    "3. - Jedna z najprostszych metod optymalizacji\n",
    "   - polega na wyznaczenia gradientu funkcji kosztu -> najpierw liczmy funkcję kosztu a potem gradient z tej funkcji\n",
    "    - ten gradient mówi o kierunku najszybszej zmiany wartości tej funkcji.\n",
    "    - następnie można korygować parametry (jak mocno określa learning rate)\n",
    "    - aktualizację parametrów robimy iteracyjnie, aż będzie znikoma zmiana parametrów.\n",
    "    Warianty metody gradientu prostego:\n",
    "      - podstawowa - gradient liczony na podstawie całego zestawu danych\n",
    "      - stochastyczna - gradient liczony przy każdej iteracji na podstawie losowej próbki\n",
    "      - batch - coś pomiędzy podstawowe a stochastycznej, gradient liczony przy danej iteracji na podstawie części zestawu danych "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9adf78665426d0efa49c5c655146a56a62f3e0076b82afcedd4d75df05ce69d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
