{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "<font size=\"5\">\n",
    "\n",
    "Laboratorium z przedmiotu: \\\n",
    "**Głębokie uczenie i analiza obrazów**\n",
    "\n",
    "Ćwiczenie 1: \\\n",
    "**Regresja logistyczna i inne klasyczne algorytmy klasyfikacji**\n",
    "\n",
    "</font>\n",
    "\n",
    "\\\n",
    "Marta Szarmach \\\n",
    "Zakład Telekomunikacji Morskiej \\\n",
    "Wydział Elektryczny \\\n",
    "Uniwersytet Morski w Gdyni\n",
    "\n",
    "07.2023\n",
    "</div>\n",
    "\n",
    "\n",
    "# 1. Wprowadzenie\n",
    "\n",
    "**Uczenie maszynowe** jest to zbiór algorytmów, które odnajdują zależności ukryte w danych, potrafią te zależności modelować (tj. opisywać za pomocą matematycznych struktur) bez bycia jawnie zaprogramowanym przez człowieka i doskonalą swoje działanie (tj. potrafią dobierać lepsze parametry opisujące modele) dzięki dostarczeniu do nich nowych danych (uczenie maszynowe jest jedynie częścią szerszego zagadnienia, jakim jest **sztuczna inteligencja**, tj. dziedzina nauki z pogranicza informatyki, matematyki, kongwinistyki i neurologii, zajmująca się tworzeniem maszyn/oprogramowania ,,udających'' ludzką inteligencję, tj. potrafiącego analizować dostarczone doń dane, wyciągać na ich podstawie wnioski i podejmować decyzje).  Innymi słowy, w przeciwieństwie do klasycznego programowania, nie tworzy się gotowych reguł, lecz algorytm sam, na podstawie dostarczonych danych i spodziewanych odpowiedzi na nie, określa reguły, tzn. tworzy **model**, który stara się odzwierciedlić strukturę danych i sposób wyznaczania tychże odpowiedzi. Model najczęściej opisany jest za pomocą fukcji (hipotezy) zależnej od **parametrów** $\\theta$, które są pewnymi wartościami liczbowymi ulegającymi zmianie w ramach treningu (nie należy mylić parametrów z **hiperparametrami**, które również opisują w pewnym sensie model, lecz nie są wyznaczane podczas treningu, a wręcz przeciwnie, muszą być podane wcześniej).\n",
    "\n",
    "Aby stworzyć i wytrenować model, najczęściej określa się pewną funkcję (nazywaną **funkcją kosztu** $J(\\theta)$), która jest w stanie policzyć, jak bardzo model myli się podczas dokonywania predykcji. **Trening** polega na optymalizacji funkcji kosztu, a dokładniej mówiąc, iteracyjnej aktualizacji parametrów modelu, aż koszt na danych treningowych staje się (najczęściej) minimalny - wówczas uważa się, że model możliwie najlepiej odzwierciedla te dane. Jedną z metod optymalizacji jest **metoda gradientu prostego**, której działanie opiera się na poszukiwaniu lokalnego minimum poprzez wyznaczanie gradientu $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ (kierunku najszybszej zmiany) funkcji kosztu i aktualizacji parametrów zgodnie z tym gradientem. \n",
    "\n",
    "\\\n",
    "Uczenie maszynowe można podzielić na dwie podstawowe grupy: \n",
    "* **uczenie nadzorowane**, w którym każdy rekord danych dostarczony do algorytmu posiada **etykietę** zawierającą pożądaną odpowiedź algorytmu na ten konkretny rekord. Algorytmy uczenia nadzorowanego potrafią rozwiązywać takie problemy jak:\n",
    "    * **regresja** - przypisanie do rekordu danych pewnej dowolnej liczby,\n",
    "    * **klasyfikacja** -  przypisanie do rekordu danych liczby (z przedziału dyskretnego), symbolizującej jego przynależność do pewnej klasy,\n",
    "* **uczenie nienadzorowane** - dane nie posiadają predefiniowanych etykiet, algorytmy muszą same znaleźć strukturę w danych. Rozwiązują takie problemy, jak m.in.:\n",
    "    * **grupowanie** - podział zebranych danych na grupy, tak, aby dane z jednej grupy były bardziej podobne do siebie niż do danych z innych grup,\n",
    "    * **wykrywanie anomalii** - odnalezienie w zbiorze danych tych rekordów, które w pewien sposób odróżniają się od reszty.\n",
    "\n",
    "W niniejszym ćwiczeniu zajmować się będziemy jedynie zagadnieniem klasyfikacji. Niektórymi algorytmami uczenia maszynowego realizującymi klasyfikację są:\n",
    "* **Maszyna wektrów wspierających** (ang. *Support Vector Machine*, **SVM**) - algorytm, który dokonuje klasyfikacji danych poprzez utworzenie (z pomocą dodatkowych funkcji, tzw. kerneli) dodatkowego wymiaru i hiperpłaszczyzny, która oddziela na tym wymiarze dane z różnych klas z maksymalnym możliwym marginesem,\n",
    "* **Drzewo decyzyjne** (ang. *Decision Tree*) - zbiór hierarchicznie następujących po sobie instrukcji warunkowych, których ostatnia warstwa decyduje o wyniku predykcji,\n",
    "* *k* **najbliższych sąsiadów** (ang. *k Nearest Neighbours*, *k*-NN) - to, jaka zostanie podjęta decyzja dotycząca badanego rekordu, zależy od etykiet $k$ innych rekordów najbliższych temu rekordowi,\n",
    "* a także **regresja logistyczna** - która to będzie głównym zagadnieniem niniejszego ćwiczenia. W toku działania tegoż algorytmu, dokonuje się dopasowania pewnej funkcji, wiążącej parametry $\\theta$ i dane X, której zadaniem jest oszacowanie prawdopodobieństwa, z jakim określony rekord danych należy do pewnej klasy.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:828/1*PQ8tdohapfm-YHlrRIRuOA.gif' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cel ćwiczenia\n",
    "\n",
    "**Celem niniejszego ćwiczenia** jest zapoznanie się z działaniem klasycznych algorytmów uczenia maszynowego realizujących zagadnienia klasyfikacji poprzez:\n",
    "* implementacji ,,od zera'' algorytmu regresji logistycznej (łącznie z optymalizacją funkcji kosztu metodą gradientu prostego),\n",
    "* użycie gotowych klas z biblioteki Scikit-learn z zaimplementowanymi gotowymi klasyfikatorami (regresją logistyczną, drzewem decyzyjnym i *k*-NN) i porównanie otrzymanych wyników.\n",
    "\n",
    "\n",
    "# 3. Stanowisko laboratoryjne\n",
    "\n",
    "Do wykonania niniejszego ćwiczenia niezbędne jest stanowisko laboratoryjne, składające się z komputera klasy PC z zainstalowanym oprogramowaniem:\n",
    "* językiem programowania Python (w wersji 3.8),\n",
    "* IDE obsługującym pliki Jupyter Notebook (np. Visual Studio Code z rozszerzeniem ipykernel).\n",
    "\n",
    "\n",
    "# 4. Przebieg ćwiczenia\n",
    "## 4.1. Implementacja algorytmu regresji logistycznej ,,od zera''\n",
    "\n",
    "### Inicjalizacja: import niezbędnych elementów\n",
    "\n",
    "Na początku wykonaj poniższy fragment kodu, aby zaimportować biblioteki niezbędne do wykonania poniższego ćwiczenia:\n",
    "* **Scikit-learn** - biblioteka zawierająca gotowe implementacje wielu algorytmów klasycznego uczenia maszynowego, a także zbiory danych czy metryki. Tutaj skorzystamy ze zbioru danych iris - `datasets.load_iris`.\n",
    "* **NumPy** - biblioteka umożliwiająca wykonywanie wysoko zoptymalizowanych obliczeń matematycznych na objektach typu *numpy array* (wielowymiarowych tablic).\n",
    "* **Matplotlib** - biblioteka wspomagająca wizualizację pracy czy analizę danych poprzez wyświetlanie wykresów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.126027300Z",
     "start_time": "2023-10-17T08:19:38.138100500Z"
    }
   },
   "outputs": [],
   "source": [
    "#! python -m pip install numpy==1.22.3\n",
    "#! python -m pip install scikit-learn==0.24.2\n",
    "#! python -m pip install matplotlib==3.4.2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zaimportowaniu niezbędnych bibliotek, załadujmy gotowy zbiór danych Iris (zawierający dane o wymiarach różnych rodzajów irysów), pochodzący z repozytorium UCI (więcej informacji o tym zbiorze danych możesz uzyskać [TUTAJ](https://archive.ics.uci.edu/dataset/53/iris)). Dane zapiszmy pod zmienną X, a odpowiadające im etykiety - y.\n",
    "\n",
    "Ponadto, dla zobrazowania danych, które będziemy używać, wyświetlmy rozmiary tablic X i y, a także 5 pierszwych rekordów/etykiet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.252274300Z",
     "start_time": "2023-10-17T08:19:41.126027300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych wejściowych: (100, 4)\n",
      "Przykładowe dane wejściowe: \n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Wymiary etykiet: (100,)\n",
      "Przykładowe etykiety: \n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[0:100,:]\n",
    "y = iris.target[0:100]\n",
    "print(\"Wymiary danych wejściowych: \" + str(X.shape))\n",
    "print(\"Przykładowe dane wejściowe: \")\n",
    "print(X[0:5,:])\n",
    "print(\"Wymiary etykiet: \" + str(y.shape))\n",
    "print(\"Przykładowe etykiety: \")\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się wyświetlonym informacjom na temat wczytanych danych. Wczytujemy 100 rekordów o 4 cechach (wymiary tablicy X to 100x4). Etykiety zapisane są w formie 1-wymiarowego wektora y (o wymiarze 100) - z dokumentacji możemy się dowiedzieć, że mogą one przyjmować jedną z 3 wartości: 0, 1 lub 2, symbolizujących przynależność rekordu do jednej z 3 klas (każda klasa oznacza inny typ irysa), lecz w naszym przypadku, aby skupić się jedynie na zagadnieniu binarnej klasyfikacji, podczas ładowania danych odrzucamy ostatnich 50 rekordów należących do 3 klasy, więc operujemy jedynie na 2 klasach.\n",
    "\n",
    "<font size=\"2\">Informacje o wymiarach otrzymanych tablic będą nam bardzo potrzebne na późniejszym etapie, podczas pracy na macierzach przy implementacji procesu terningu i predykcji naszego algorytmu.</font>\n",
    "\n",
    "\n",
    "### Przygotowanie danych\n",
    "\n",
    "Aby móc w pełni korzystać z tych danych, musimy je jednak nieco przekształcić. W tym celu wykonamy dwie operacje:\n",
    "* dokonamy **standaryzacji** danych, tj. przekształcimy je tak, aby bez zmiany ich struktury, każda z cech posiadała średnią o wartości 0 i wariancję o wartości 1 - poprawi to działanie algorytmów uczenia maszynowego, zwłaszcza podczas pracy nad danymi o szerokiej dynamice (różnych rzędach wielkości) czy różnych jednostkach,\n",
    "* podzielimy cały dostępny zbiór danych na 2 zestawy: \n",
    "    * **treningowy** - na podstawie którego model zostanie wytrenowany (to pod te dane zostaną dopasowane parametry naszego modelu),\n",
    "    * **testowy** - który posłuży nam do określenia, jak dobrze działa nasz model na danych, których wcześniej model nie widział.\n",
    "\n",
    "<font size=\"2\">W tym konkretnym przypadku nie będziemy jeszcze wydzielać trzeciego z zazwyczaj tworzonych zestawów danych, zestawu **walidacyjnego**, na podstawie którego dopasowuje się hiperparametry modelu zanim przejdzie się do jego właściwego uczenia - nie ma takiej potrzeby, gdyż nie będziemy w ramach tego ćwiczenia zajmować się dopasowaniem żadnych hiperparametrów.</font>\n",
    "\n",
    "Zacznijmy od napisania funkcji `standarize_data` służącej do normalizacji danych. Zgodnie z poniższym wzorem, zaimplementuj funkcję, która przyjmuje jako argument wejściowe (tablicę X), oblicza średnią $\\mu_n$ i odchylenie standardowe $\\sigma_n$ każdej z cech $x_n$, a następnie odejmuje od nich wyliczone średnie i dzieli je przez odchylenia stardardowe: \n",
    "\\begin{equation*}\n",
    "\tx_n = \\frac{x_n - \\mu_n}{\\sigma_n}\n",
    "\\end{equation*}\n",
    "po czym zwraca tak znormalizowane dane jako tablicę Xnorm.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `standarize_data`:\n",
    "* Zapoznaj się z dokumentacją dwóch funkcji z biblioteki NumPy: `np.mean` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)) oraz `np.std` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.std.html)). Zwróć uwagę na parametr `axis`.\n",
    "* Zastosuj mechanizm broadcastigu występujący podczas operacji na tablicach z użyciem NumPy (więcej o tym możesz przeczytać [TUTAJ](https://numpy.org/doc/stable/user/basics.broadcasting.html)).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.283529600Z",
     "start_time": "2023-10-17T08:19:41.189785400Z"
    }
   },
   "outputs": [],
   "source": [
    "def standarize_data(X):\n",
    "    \"\"\"Funkcja realizująca standaryzację danych: przyrównanie średniej każdej z cech do 0, \n",
    "    a wariancji do 1. \\n\n",
    "    Argument: X - nieprzekształcone dane (numpy array, shape = (num_samples, num_features) ). \\n\n",
    "    Zwraca: \\n\n",
    "    - Xnorm - znormalizowane dane (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    - mu - wektor ze średnimi każdej z cech (numpy array, shape = (num_features,) ), \\n\n",
    "    - sigma - wektor z odchyleniami standardowymi każdej z cech (numpy array, shape = (num_features,) ).\"\"\"\n",
    "\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz mu - średnią każdej z cech (pamiętaj, że cechy przechowywane są w każdej kolumnie tablicy X)\n",
    "    mu = np.mean(X, axis=0)\n",
    "    # Oblicz sigma - odchylenie standardowe każdej z cech\n",
    "    sigma = np.std(X, axis=0)\n",
    "    # Oblicz Xnorm - odejmij średnią od każdej z cech i podziel ją przez jej odchylenie standardowe \n",
    "    # (możesz to zrobić w 1 linijce kodu)\n",
    "    Xnorm = np.divide(np.subtract(X, mu), sigma)\n",
    "    # -------------------------------\n",
    "    \n",
    "    return Xnorm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz przejdźmy do utworzenia funkcji `split_data`, która ma za zadanie podzielić dostarczony jej zbiór danych X (i etykiety y) na zestaw treningowy i testowy, zgodnie z podanym jako argument procentem (wartość `percentage_train` odpowiadać ma procentowi, jaką częścią oryginalnego zbioru danych ma być zbiór treningowy).\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `split_data`:\n",
    "* Zapoznaj się z dokumentacją funkcji `np.random.choice` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)). Zwróć uwagę na parametr `replace`.\n",
    "* Zapoznaj się z trickiem na usunięcie elementów jednej listy z drugiej z wykorzystaniem różnicy zbiorów (więcej o tym możesz przeczytać [TUTAJ](https://stackoverflow.com/questions/3428536/how-do-i-subtract-one-list-from-another/)). Do utworzenia listy indeksów wszystkich rekordów danych możesz użyć funkcji `np.arange` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.arange.html)).\n",
    "* Zapoznaj się ze sposobami na wyodrębnienie części tablicy w NumPy ([TUTAJ](https://numpy.org/doc/stable/user/basics.indexing.html)). Możesz je filtrować według elementów innej listy!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.283529600Z",
     "start_time": "2023-10-17T08:19:41.236662300Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(X,y, percentage_train = 70.0):\n",
    "    \"\"\"Funkcja dzieląca losowo dane na zestaw treningowy oraz testowy w zadanej proporcji. \\n\n",
    "    Argumenty: \\n\n",
    "    - X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    - y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    - percentage_train (argument opcjonalny) - jakim procentem wejściowych danych ma być \n",
    "      zestaw treningowy (skalar, float, domyślna wartość: 70). \\n\n",
    "    Zwraca: \\n\n",
    "    - Xtrain - dane treningowe (numpy array, shape = (num_samples * percentage_train, num_features) ), \\n\n",
    "    - ytrain - etykiety do danych treningowych (numpy array, shape = (num_samples * percentage_train,) ), \\n\n",
    "    - Xtest - dane testowe (numpy array, shape = (num_samples * (100-percentage_train), num_features) ), \\n\n",
    "    - ytest - etykiety do danych testowych (numpy array, shape = (num_samples * (100-percentage_train),) ).\"\"\"\n",
    "    \n",
    "    np.random.seed(100) # Dla zapanowania nad \"losowością\"\n",
    "    num_all_datapoints =  X.shape[0] # Ilość wszystkich danych\n",
    "    num_train_datapoints = int(np.round(X.shape[0]*percentage_train/100)) # Docelowa wielkość zestawu treningowego\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Znając rozmiar danych wejściowych, wygeneruj indices_train - listę indeksów tych rekordów, \n",
    "    # które mają należeć do zestawu treningowego\n",
    "    indices_train = np.random.choice(num_all_datapoints, num_train_datapoints, replace=False)\n",
    "    # Wszystkie indeksy, które nie należą do indices_train, zapisz do indices_test\n",
    "    indices_test = list(set(np.arange(X.shape[0])) - set(indices_train))\n",
    "    # Wygeneruj zmienne z właściwie podzielonym zbiorem danych: Xtrain, ytrain, Xtest, ytest\n",
    "    Xtrain = X[indices_train,:]\n",
    "    ytrain = y[indices_train]\n",
    "    Xtest = X[indices_test,:]\n",
    "    ytest = y[indices_test]\n",
    "    # ------------------------------\n",
    "    \n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pora sprawdzić, jak działają napisane przez Ciebie funkcje! Uruchom poniższy kod, aby podzielić nasz zbiór danych na zestaw treningowy (powinien domyślnie zawierać 70 elementów) i testowy (pozostałe 30 elementów). Elementy w poszczególnych cechach powinny po normalizacji oscylować wokół 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.361652700Z",
     "start_time": "2023-10-17T08:19:41.252274300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych treningowych: (70, 4)\n",
      "Wymiary danych testowych: (30, 4)\n",
      "Przykładowe znormalizowane dane treningowe: \n",
      "[[-0.97023354  1.12689892 -1.11317579 -1.32513741]\n",
      " [ 0.73542816 -1.9493154   0.67340264  0.2589927 ]\n",
      " [-0.81517338  0.68743973 -0.97574668 -0.79709404]\n",
      " [-1.59047416 -1.7295858  -1.18189035 -0.9731085 ]\n",
      " [-0.81517338  0.24798054 -1.2506049  -1.14912295]]\n"
     ]
    }
   ],
   "source": [
    "# Podział danych na część treningową i testową\n",
    "Xtrain, ytrain, Xtest, ytest = split_data(X,y)\n",
    "print(\"Wymiary danych treningowych: \"+str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \"+str(Xtest.shape))\n",
    "\n",
    "# Normalizacja obu zestawów danych\n",
    "Xtrain_norm, _, _ = standarize_data(Xtrain) # pomijamy zwracanie mu i sigma dla danych treningowych\n",
    "Xtest_norm, mu_test, sigma_test = standarize_data(Xtest)\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening modelu regresji logistycznej\n",
    "\n",
    "Teraz, kiedy mamy już gotowe dane, na których możemy pracować, przejdźmy do najważniejszej rzeczy, czyli napisania funkcji, które utworzą nasz model oparty na regresji logistycznej i pozwolą mu się uczyć!\n",
    "\n",
    "Przypomnijmy, że **hipotezą** $h_\\theta(x)$ (tj. funkcją, która wiąże parametry modelu i dane wejściowe, dając w wyniku predykcje) regresji logistycznej jest: \n",
    "\\begin{equation*}\n",
    "    h_\\theta(x) = g(\\theta^Tx)\n",
    "\\end{equation*}\n",
    "gdzie funkcja $g(z)$ jest to tzw. funkcja **sigmoid** o następującej postaci:\n",
    "\\begin{equation*}\n",
    "    g(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation*}\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1280/1*OUOB_YF41M-O4GgZH_F2rw.png' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>\n",
    "\n",
    "Funkcja sigmoid ma kilka ciekawych właściwości, dzięki którym jest często spotykana przy rozwiązywaniu problemów klasyfikacji: jej wartości zawierają się w przedziale od 0 do 1 (dlatego można je traktować jak prawdopodobieństwo należenia danego rekordu danych do pewnej klasy i np. traktować te dane, dla których sigmoid zwrócił wartość większą niż 0,5, jako należące do tejże klasy), a także jest odwracalna i różniczkowalna, dzięki czemu da się obliczać jej gradient niezbędny w procesie uczenia modelu. Zaimplementuj zatem funkcję `sigmoid`, która zwraca wartość sigmoidu dla dowolnej *numpy array*!\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `sigmoid`: Warto skorzystać z funkcji `np.exp` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.exp.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.361652700Z",
     "start_time": "2023-10-17T08:19:41.299154400Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Funkcja obliczająca wartość sigmoidu dla zadanego argumentu z. \\n\n",
    "    \"\"\"\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    # ------------------------------\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `sigmoid`. Obliczymy, co zwraca ta funkcja, gdy argumentem są same zera: skalar, wektor 1-D oraz macierz 2-D. W każdym przypadku, sigmoid powinien zwrócić stukturę o takich samych wymiarach, jak dane wejściowe, a każdy z jej elementów powinien wynosić 0,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.549148800Z",
     "start_time": "2023-10-17T08:19:41.346026800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid dla skalara: 0.5\n",
      "Sigmoid dla wektora: [0.5 0.5 0.5]\n",
      "Sigmoid dla macierzy: [[0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji sigmoidu\n",
    "print(\"Sigmoid dla skalara: \" + str(sigmoid(0)))\n",
    "print(\"Sigmoid dla wektora: \" + str(sigmoid(np.zeros((3)))))\n",
    "print(\"Sigmoid dla macierzy: \" + str(sigmoid(np.zeros((3,3)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako funkcję kosztu $J(\\theta)$ użyjemy **binarnej entropii krzyżowej** (ang. *Binary Cross Entropy*, BCE), często spotykaną przy okazji problemów klasyfikacji binarnej. Jej wartość jest tym większa, im więcej pomyłek popełni klasyfikator: przy zgodności etykiety $y^{(i)}$ i predykcji $h_\\theta(x^{(i)})$, oba człony wyrażenia zerują się. Funkcja ta opisana jest wzorem:\n",
    "\\begin{equation*}\n",
    "\tJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)}\\log(h_\\theta(x^{(i)})) - (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]\t\n",
    "\\end{equation*}\n",
    "\n",
    "Jak już wiesz, trening modelu opiera się na znalezieniu optymalnych parametrów $\\theta$, tj. takich, przy których funkcja kosztu jest minimalna. My taką optymalizację przeprowadzimy z wykorzystaniem metody gradientu prostego, która do poprawnego działania musi znać gradient $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ naszej funcji kosztu. Pamiętaj, że **gradient ma taki sam wymiar, jak wektor parametrów $\\theta$**, a zatem składa się z $n$ elementów. W przypadku gradientu fukcji BCE, każdy z jego $n$ elementów można obliczyć z następującego wzoru (pomijam tutaj jego wyprowadzenie):\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial J(\\theta)}{\\partial \\theta_n} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)}) \\cdot x^{(i)}_n\n",
    "\\end{equation*}\n",
    "\n",
    "Napisz zatem funkcję `compute_cost_and_gradient`, w której na podstawie danych treningowych i zadanych parametrów $\\theta$, obliczysz koszt BCE i jego gradient.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `compute_cost_and_gradient`:\n",
    "* Przy obliczaniu kosztu, będzie Ci na pewno potrzebna funkcja `np.log2` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.log2.html)) oraz napisana przez Ciebie wcześniej funkcja `sigmoid`.\n",
    "* Zamiast używania pętli `for` do iteracji po wszystkich $m$ elementach, możesz wykonać operacje na macierzach (z wykorzystaniem funkcji `np.dot` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.dot.html))). Pamiętaj jednak o właściwościach mnożenia macierzy - nie każde macierze da się przemnożyć, muszą one mieć zgodne \"wewnętrze\" wymiary (ilość kolumn pierwszej macierzy musi być taka sama, jak ilość wierszy drugiej macierzy; wówczas w wyniku mnożenia otrzymujemy macierz o zgodnych \"zewnętrznych\" wymiarach, tj. o liczbie wierszy jak pierwsza macierz i liczbie kolumn jak druga macierz: [M,N]x[N,1]=[M,1]): w razie potrzeby zmień kolejność mnożonych macierzy albo dokonaj ich transpozycji z użyciem funkcji `np.transpose` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)).\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.580401100Z",
     "start_time": "2023-10-17T08:19:41.392909Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_cost_and_gradient(X, y, theta):\n",
    "    \"\"\"Funkcja obliczająca koszt BCE dla regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    theta - zestaw parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: \\n\n",
    "    J - obliczony koszt (skalar, float), \\n\n",
    "    grad - obliczony gradient funkcji kosztu (numpy array, shape=(num_features) ). \"\"\"\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz sigmoid\n",
    "    #print(\"X\", X) # 4x100\n",
    "    #print(\"Y\", y) # 1D - 100 elements\n",
    "    #print(\"theta\", theta) 1D - 4 elements\n",
    "    \n",
    "    sig = sigmoid(np.dot(X, np.transpose(theta)))\n",
    "    \n",
    "    # Oblicz wartość funkcji kosztu\n",
    "    J = (-np.dot(y, np.log2(sig))\n",
    "          - np.dot((1-y), np.log2(1-sig))) / X.shape[0]\n",
    "    \n",
    "    # Oblicz jej gradient\n",
    "    grad = np.dot(np.transpose(X), sig - y) / X.shape[0]\n",
    "    # ------------------------------\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy poprawność implementacji powyższej funkcji: uruchom następującą komórkę, aby wyliczyć przykładowe wartości kosztu i jego gradientu dla zerowych parametrów (kiedy wszystkie elementy $\\theta$ są równe 0), liczone dla całego naszego zbioru danych. Koszt powinien wynosić 1, a gradient [-0.2325,  0.1645, -0.6995, -0.27]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.580401100Z",
     "start_time": "2023-10-17T08:19:41.424153100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowy koszt i jego gradient, liczony dla zerowych parametrów: (1.0, array([-0.2325,  0.1645, -0.6995, -0.27  ]))\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji funkcji kosztu\n",
    "print(\"Przykładowy koszt i jego gradient, liczony dla zerowych parametrów: \"+str(compute_cost_and_gradient(X,y,np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając wyznaczoną funkcję kosztu, możemy przejść do jej optymalizacji, czyli doboru takich wartości parametrów $\\theta$, które dają najniższą wartość kosztu (tj. najlepiej odwzorowują dane treningowe). Jak już wspomniano, zrobimy to z wykorzystaniem metody gradientu prostego, według której aktualizacja parametrów odbywa się według poniższego wzoru:\n",
    "\\begin{equation*}\n",
    "    \\theta_n := \\theta_n - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_n}\n",
    "\\end{equation*}\n",
    "gdzie hiperparametr $\\alpha$ (którego wartość musi być zdefiniowana zanim przejdzie się do treningu modelu), oznacza, jak bardzo gradient funkcji kosztu ma wpływ na nową, zaktualizowaną postać parametrów $\\theta$\n",
    "\n",
    "Napisz zatem funkcję `train_logistic_regression`, w której na podstawie danych treningowych, iteracyjnie liczona jest wartość funkcji kosztu i jego gradient, parametry są aktualizowane zgodnie z metodą gradientu prostego, a ponadto przy każdej itaracji wizualizowana jest nowa wartość kosztu, aby móc ocenić, czy w ramach treningu koszt rzeczywiście spada (ważne - jeśli zaobserwowalibyśmy wzrost kosztu wraz z kolejnymi  iteracjami, oznacza to, że model CORAZ GORZEJ radzi sobie z analizą danych treningowych, a zatem wcale się nie uczy!). \"Szkielet\" tej funkcji został już napisany, wykonanych zostanie 400 iteracji, a stała uczenia może zostać w postaci domyślnej (ustawionej na 0,01).\n",
    "\n",
    "<font size=\"2\">Poprawność implementacji tej funkcji sprawdzimy nieco później.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.580401100Z",
     "start_time": "2023-10-17T08:19:41.486653300Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_logistic_regression(X,y,alpha=0.01):\n",
    "    \"\"\"Funkcja realizująca optymalizację funkcji kosztu dla regresji logistycznej\n",
    "    w celu wytrenowania modelu (otrzymania zestawu najlepszych parametrów, theta)\n",
    "    z wykorzystaniem metody gradientu prostego. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane treningowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    alpha (opcjonalnie) - stała uczenia (skalar, float, domyślnie 0.001). \\n\n",
    "    Zwraca: theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \"\"\"\n",
    "    \n",
    "    # Inicjalizacja\n",
    "    num_iterations = 400 # tyle razy wykonać ma się gradient descent\n",
    "    theta = np.zeros((X.shape[1])) # wstępna inicjalizacja parametrów samymi zerami\n",
    "    Js = np.zeros(num_iterations) # wektor przechowujący dotychczasowe wartości kosztu (do wizualizacji)\n",
    "    \n",
    "    # Uruchomienie metody gradientów prostych\n",
    "    print(\"\\nTrwa trening modelu... \")\n",
    "    for i in range(num_iterations):\n",
    "        # ------- UZUPEŁNIJ KOD --------\n",
    "        # Korzystając z wcześniej napisanej funkcji, oblicz funkcję kosztu J i jej gradient grad\n",
    "        J, grad = compute_cost_and_gradient(X, y, theta)\n",
    "        # Zapisz obliczony koszt jako odpowiedni element w Js\n",
    "        Js[i] = J\n",
    "        # Dokonaj aktualizacji parametrów theta o wcześniej obliczony gradient przemnożony przez stałą uczenia\n",
    "        theta = theta - alpha * grad\n",
    "        # ------------------------------\n",
    "    print(\"Zakończono. \")\n",
    "    \n",
    "    # Wizualizacja zmian kosztu\n",
    "    plt.figure()\n",
    "    plt.plot(Js)\n",
    "    plt.title(\"Efekty treningu modelu - zmiany w koszcie\")\n",
    "    plt.xlabel(\"Numer iteracji\")\n",
    "    plt.ylabel(\"Wartość funkcji kosztu\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predykcja i poskładanie wszystkiego w całość!\n",
    "\n",
    "Wytrenowany model musi umieć dokonywać predykcji - w tym przypadku, podejmować decyzję, czy analizowany rekord danych zaklasyfikować do klasy pierwszej (etykieta 0) czy drugiej (etykieta 1). Napisz zatem ostatnią w tej części ćwiczenia funkcję, `predict_logistic_regression`, która oblicza funkcję hipotezy dla regresji logistycznej i zwraca odpowiednie etykiety.\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `predict_logistic_regression`: Pamiętaj, że model ma zwrócić etykietę 0 w sytuacji, kiedy prawdopodobieństwo obliczone z wykorzystaniem hipotezy jest mniejsze niż 0,5. Poszukaj funkcji z biblioteki NumPy, która realizuje przybliżanie do najbliższej liczby naturalnej!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.627278500Z",
     "start_time": "2023-10-17T08:19:41.533526Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_logistic_regression(X, theta):\n",
    "    \"\"\"Funkcja obliczająca ostateczną predykcję regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: pred - dokonane predykcje (numpy array, shape = (num_samples,) ). \"\"\"\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    pred = np.round(sigmoid(np.dot(X, np.transpose(theta))))\n",
    "    # ------------------------------\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `predict_logistic_regression`. W poniższej komórce wykonujemu tę funkcję dla pierwszych pięciu rekordów oryginalnego zbioru danych i zerowych parametrów. Powinieneś otrzymać w wyniku same zera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:41.861651100Z",
     "start_time": "2023-10-17T08:19:41.549148800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowe predykcje przy zerowych parametrach: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji predykcji\n",
    "print(\"Przykładowe predykcje przy zerowych parametrach: \"+str(predict_logistic_regression(X[0:5,:],np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas uruchomić całość - od właściwego uczenia naszego modelu, aż po dokonanie przezeń predykcji na danych testowych! Uruchom poniższy kod, w którym wykorzystujemy niemal wszystko, co do tej pory udało nam się napisać. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:42.033534300Z",
     "start_time": "2023-10-17T08:19:41.580401100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trwa trening modelu... \n",
      "Zakończono. \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność modelu na danych testowych: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Trening modelu (na danych treningowych)\n",
    "theta = train_logistic_regression(Xtrain_norm,ytrain)\n",
    "# Dokonanie predykcji (na danych testowych) i obliczenie dokładności modelu\n",
    "pred = predict_logistic_regression(Xtest_norm,theta)\n",
    "accuracy = np.mean(pred==ytest)\n",
    "print(\"Dokładność modelu na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli na wykresie obserwujesz stopniowy spadek kosztu - świetnie, Twój model się uczy! Można też zaobserwować bardzo wysoką wartość dokładności (**dokładność** to procent decyzji, które model podjął właściwie) naszego modelu na danych z zestawu testowego, świadczącą o jego poprawnym działaniu. \n",
    "\n",
    "\n",
    "## 4.2. Uruchomienie klasyfikatorów z biblioteki Scikit-learn\n",
    "\n",
    "### Import nowych niezbędnych klas\n",
    "\n",
    "Udało Ci się stworzyć model klasyfikatora opartego o regresję logistyczną od zera. Na szczęście nie zawsze trzeba włożyć tyle pracy, aby móc używać algorytmów uczenia maszynowego. Istnieją biblioteki posiadające gotowe implementacje wielu z nich. Jedną z takich bibliotek jest wykorzystywana już przez Ciebie biblioteka Scikit-learn. Tym razem do sprawdzenia, jak z analizowanym tutaj zbiorze danych iris radzą sobie inne algorytmy, wykorzystamy jej gotowe klasy i metody:\n",
    "- model regresji logistycznej - `linear_model.LogisticRegression` (dokumentacja [TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)), \n",
    "- model drzewa decyzyjnego - `tree.DecisionTreeClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)),\n",
    "- model *k*-NN - `neighbors.KNeighborsClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)),\n",
    "- metrykę dokładności - `metrics.accuracy_score` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)),\n",
    "- klasę standaryzującą dane - `preprocessing.StandardScaler` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)),\n",
    "- funkcję dzielącą dane na zestaw treningowy i testowy - `model_selection.train_test_split` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "\n",
    "Uruchom poniższą komórkę, aby zaimportować niezbędne klasy i metody. Zapoznaj się też z ich dokumentacją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:42.431119600Z",
     "start_time": "2023-10-17T08:19:42.033534300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych z użyciem gotowych metod\n",
    "\n",
    "Mając wciąż w pamięci przechowywane zmienne X i y z oryginalnym, jeszcze nieznormalizowanym ani niepodzielonym zbiorem danych iris (oraz etykietami), możemy je ponownie wykorzystać w tej części ćwiczenia. Napisz więc fragment kodu, który:\n",
    "* dzieli dane X i y na właściwe zestawy Xtrain i Xtest (użyj metody `train_test_split`) - niech zestaw treningowy zawiera 70% danych,\n",
    "* standaryzuje Xtrain i Xtest (zwróć uwagę na metodę `fit_transform` klasy `StandardScaler`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:42.540478700Z",
     "start_time": "2023-10-17T08:19:42.431119600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych treningowych: (70, 4)\n",
      "Wymiary danych testowych: (30, 4)\n",
      "Przykładowe znormalizowane dane treningowe: \n",
      "[[ 1.0946559  -0.18510948  1.29436799  1.20552424]\n",
      " [ 0.93401458 -0.4047309   1.22380276  1.38739212]\n",
      " [-0.6723987  -1.72245943  0.37701996  0.47805272]\n",
      " [ 2.37978653  0.03451194  1.50606369  1.38739212]\n",
      " [ 0.29144927 -0.4047309   0.58871566  1.02365636]]\n"
     ]
    }
   ],
   "source": [
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Podziel dane z X i y na zestaw treningowy (70%) i testowy (30%) z wykorzystaniem metody train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)\n",
    "# Dokonaj normalizacji danych treningowych z wykorzystaniem klasy StandardScaler - zwróć Xtrain_norm\n",
    "scaler = StandardScaler()\n",
    "Xtrain_norm = scaler.fit_transform(Xtrain)\n",
    "# Dokonaj normalizacji danych testowych - zwróć Xtest_norm\n",
    "Xtest_norm = scaler.fit_transform(Xtest)\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Wymiary danych treningowych: \"+str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \"+str(Xtest.shape))\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresja logistyczna z biblioteki Scikit-learn\n",
    "\n",
    "Mamy przygotowane dane, pora zatem wytrenować model regresji logistycznej z wykorzystaniem klasy `LogisticRegression`!\n",
    "* Do trenowania (na bazie zestandaryzowanego zestawu treningowego) użyj metody `fit`.\n",
    "* Do dokonania predykcji użyj metody `predict` - zobacz, jakich predykcji dokona Twój model na widok danych z zestandaryzowanego zestawu testowego (zapisz je do zmiennej o nazwie `pred`).\n",
    "* Oblicz dokładność predykcji Twojego modelu (dla zestandaryzowanych danych testowych) z wykorzystaniem metody `accuracy_score`. Wynik zapisz do zmiennej o nazwie `acccuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:42.556120200Z",
     "start_time": "2023-10-17T08:19:42.477978100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność modelu na danych testowych: 100.0%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression() # zostawmy domyślne ustawienia\n",
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Wytrenuj model regresji logistycznej na zestandaryzowanych danych treningowych\n",
    "model.fit(Xtrain_norm, ytrain)\n",
    "# Zwróć predykcję modelu (do zmiennej o nazwie pred) dla  zestandaryzowanych danych testowych\n",
    "pred = model.predict(Xtest_norm)\n",
    "# Oblicz dokładność predykcji\n",
    "accuracy = accuracy_score(ytest, pred)\n",
    "# ----------------------------------------\n",
    "print(\"Dokładność modelu na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T08:19:42.587354700Z",
     "start_time": "2023-10-17T08:19:42.524857600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność drzewa decyzyjnego na danych testowych: 100.0%\n",
      "Dokładność k-NN na danych testowych: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krzysiu\\anaconda3\\envs\\ai_3_8\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Drzewo decyzyjne\n",
    "model2 = DecisionTreeClassifier(max_depth=5)\n",
    "model2.fit(Xtrain_norm,ytrain)\n",
    "pred = model2.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność drzewa decyzyjnego na danych testowych: \"+str(accuracy*100)+'%')\n",
    "\n",
    "# k-NN\n",
    "model3 = KNeighborsClassifier(n_neighbors=3)\n",
    "model3.fit(Xtrain_norm,ytrain)\n",
    "pred = model3.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność k-NN na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inne klasyfikatory dostępne w Scikit-learn\n",
    "\n",
    "Na sam koniec, sprawdź, jak zachowują się inne modele klasyfikatorów, dostępne w ramach biblioteki Scikit-learn: drzewo decyzyjne oraz *k*-NN! Uruchom po prostu poniższy kod:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gratulacje! W ten sposób zakończyliśmy to ćwiczenie, w którym skupiliśmy się na działaniu klasycznych algorytmów klasyfikacji.\n",
    "\n",
    "\n",
    "## 5. Pytania kontrolne\n",
    "\n",
    "1. Czym różni się uczenie nadzorowane od nienadzorowanego?\n",
    "2. Jakie są różnice pomiędzy problemem regresji a klasyfikacji?\n",
    "3. Opisz krótko, na czym polega trening modelu uczenia maszynowego z wykorzystaniem metody gradientów prostych."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. **Uczenie nadzorowane** - do każdego rekordu dostarczamy etykietę (odpowiedź, czyli dane wyjściowe). Tutaj celem jest nauczenie modelu przewidywania wyjść na podstawie nowych danych wejściowych. Rozwiązuje problemy regresji i klasyfikacji.\n",
    "   **Uczenie nienadzorowane** - do danych nie dostarczamy etykiet, algorytm sam ma znaleźć odpowiedź. Model ma odnaleźć nowe wzorce, struktury, odpowiedzi. Rozwiązuje problemy grupowania, redukcji wymiarów, asocjacje.\n",
    "2. **regresja** - przewidywanie wartości liczbowej. Wynik jest punktem na osi liczbowej. Równanie matematyczne.\n",
    "   **klasyfikacja** - przyporządkowanie rekordu do danej grupy, symbolizuje przynależność do jakieś klasy. Może być binarna (0, 1), wieloklasowa, wieloetykietowa.\n",
    "3. - głównym celem jest optymalizacja funkcji kosztu\n",
    "   - polega na wyznaczenia gradientu funkcji kosztu -> najpierw liczmy funkcję kosztu a potem gradient z tej funkcji\n",
    "    - ten gradient mówi o kierunku najszybszej zmiany wartości tej funkcji.\n",
    "    - następnie można korygować parametry (jak mocno określa learning rate)\n",
    "    - aktualizację parametrów robimy iteracyjnie, aż będzie znikoma zmiana parametrów.\n",
    "    Warianty metody gradientu prostego:\n",
    "      - podstawowa - gradient liczony na podstawie całego zestawu danych\n",
    "      - stochastyczna - gradient liczony przy każdej iteracji na podstawie losowej próbki\n",
    "      - batch - coś pomiędzy podstawowe a stochastycznej, gradient liczony przy danej iteracji na podstawie części zestawu danych "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9adf78665426d0efa49c5c655146a56a62f3e0076b82afcedd4d75df05ce69d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
